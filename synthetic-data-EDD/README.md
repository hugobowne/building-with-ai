# Scratch Area for Synthetic Data Generation & Evaluation

This directory contains scripts and data used for experimenting with synthetic data generation and evaluation viewer setup, mirroring parts of the main `eval` directory.

## Contents

*   `definitions.py`: Defines the user personas and scenarios as Python dictionaries (hardcoded based on original `../data/*.json` files). Can be run directly to save these definitions to `data/personas.json` and `data/scenarios.json`.
*   `synthetic_data_generator.py`: Imports definitions from `definitions.py`, uses the OpenAI API to generate synthetic questions based on these, and saves the output to `data/questions.json`.
*   `simple_llm_judge.py`: (Copied from `../`) Script that uses an LLM (like OpenAI or Gemini) to evaluate model responses based on labeled examples (pass/fail + reason).

*   `data/` directory:
    *   `personas.json` / `scenarios.json`: Contains the definitions saved by `definitions.py`.
    *   `questions.json`: Contains the synthetic questions generated by `synthetic_data_generator.py`.
    *   Response Files (`responses_*.json`, `model_comparison_*.json`): Contains responses generated using OpenAI/RAG and Gemini for the questions. *Note: These were manually added/copied for viewer testing.*
    *   Evaluated Files (`llm_evaluated_*.json`, `gemini_llm_evaluated_*.json`): Contains evaluations performed by `simple_llm_judge.py`. *Note: These were manually added/copied for viewer testing.*

*   `viewers/` directory:
    *   Contains copies of HTML viewers (e.g., `compare_viewer.html`, `evaluation_comparison.html`, `manual_evaluator.html`) moved here for testing with the data in `scratch/data/`.

## Workflow Demonstrated

This setup demonstrates an evaluation-driven development workflow for LLM applications:

1.  **Define Test Cases:** Define user personas and scenarios (`definitions.py`) representing expected usage patterns.
2.  **Generate Synthetic Data:** Create test questions automatically based on the defined personas and scenarios (`synthetic_data_generator.py`). This allows testing before having real user data.
3.  **Generate Initial Responses:** Run the synthetic questions through your initial LLM system (e.g., an OpenAI-based RAG system) to get baseline responses (results in files like `responses_*.json`).
4.  **Manual Labeling:** To build ground truth for evaluation (beyond just "vibes"), manually label a subset of the responses (e.g., 20-100 examples) as "pass" or "fail" and provide a reason. The `manual_evaluator.html` viewer aids this process.
5.  **System Iteration & Comparison:** Make changes to your system (e.g., try a different model like Gemini, adjust prompts, improve retrieval). Generate responses from the new system version for the same questions.
6.  **Raw Comparison:** Use a viewer like `compare_viewer.html` (loading `model_comparison_*.json`, which combines responses) to directly compare the outputs of different models/system versions side-by-side.
7.  **Automated Evaluation (LLM-as-Judge):** Use the manually labeled data to train or configure an LLM-as-judge (`simple_llm_judge.py`). This judge can then automatically evaluate large sets of responses consistently.
8.  **Analyze Evaluations:** Use a viewer like `evaluation_comparison.html` (loading `llm_evaluated_*.json` and `gemini_llm_evaluated_*.json`) to compare the pass/fail rates and reasons assigned by the LLM judge across different models or system versions. Identify patterns (e.g., where one model struggles) to guide further improvements.

This creates a flywheel: generate data -> evaluate -> identify weaknesses -> improve system -> re-evaluate.

*More context to be added later.* 