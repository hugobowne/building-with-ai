[
  {
    "id": "q1",
    "question": "What are the first principles for building LLM applications mentioned in Workshop 1?",
    "user_type": "student",
    "scenario": "general",
    "gemini_response": [
      "Based on the provided transcript sections, the first principles mentioned for building LLM applications are:\n\n1.  **API calls (Input/Output):** LLMs involve API calls where you send input and receive output.\n2.  **Non-determinism:** LLMs can produce different outputs even with the same input.\n3.  **Logging, monitoring, and tracing:** You need to capture data related to the LLM's operation.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n2. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n"
    ],
    "openai_response": [
      "The first principles for building LLM applications mentioned in Workshop 1 are as follows:\n\n1. Focusing on workflows, iteration, and hands-on development.\n2. Building production-grade systems that scale beyond proof of concepts.\n3. Providing a practical first principles approach.\n4. Emphasizing the value for software engineers to learn about data science and machine learning.\n5. Not being a deep dive into every LLM tool or trend.\n6. Using tools like Lama Index, Gradio, SQLite, and Dataset.\n7. Considering LLMs as non-deterministic and different from traditional software.\n8. Prioritizing logging, monitoring, and tracing for capturing data.\n9. Following a gradual rollout and iterative development process.\n10. Working across multiple platforms and adopting tools mindfully.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n2. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2478,
        "completion_tokens": 159,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 8191.991090774536,
    "openai_duration_ms": 3437,
    "gemini_tokens": 2496,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864746"
  },
  {
    "id": "q10",
    "question": "What were the main topics covered in the first workshop?",
    "user_type": "student",
    "scenario": "factual",
    "gemini_response": [
      "Based on the provided transcript sections, the main topics covered in the first workshop were:\n\n*   **Foundations of LLM software development:** Introducing GenAI and LLM development.\n*   **Overview of the software development lifecycle** in the context of LLMs.\n*   **Using key tools and frameworks:** Specifically mentioned were Lama Index, Gradio, sqlite, and the dataset tool by Simon Willison.\n*   **Setting up a foundational app:** Building an MVP application for querying PDFs and generating responses.\n*   **Iterating on prompts.**\n*   **Adding interactivity** using Gradio.\n*   **Starting logging and visualizing logs.**",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n3. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n4. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n5. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n"
    ],
    "openai_response": [
      "The main topics covered in the first workshop were:\n- Introduction to the workshop and the instructors' backgrounds\n- The incorporation of the scientific process and software in delivering value\n- The challenges of non-determinism in software with data science and machine learning\n- The importance of communication and learning from each other in the workshop\n- The focus on workflows rather than an exhaustive tool coverage\n- The introduction to GenAI and LLM development\n- Overview of the software development lifecycle\n- Introduction to key tools and frameworks for LLM-based applications\n- Setting up the foundational app for querying PDFs and generating responses\n- Guest lectures and topics for future sessions",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n3. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n4. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n5. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2529,
        "completion_tokens": 132,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:00:00.290]\nhugo bowne-anderson: Everyone.\n\nhugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n\nso\nhugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n\nBut in the meantime\nhugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n\nhugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n\nand previously I was in\nhugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n\nWhat I mean by that is\nhugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n\nhugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n\nhugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.",
            "metadata": {
              "position": 0,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:00:00.290",
              "token_count": 485
            },
            "id": "4a117ebe-0532-43df-8a26-587946cb93e4",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:50:39.550]\nh\nhugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n\nWhat I\nhugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\nStefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n\nhugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n\nhugo bowne-anderson: Everything we need in the modal form.\n\nhugo bowne-anderson: And I'll do that next time.\n\nAnd then\nhugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n\nst time\nhugo bowne-anderson:\n\nStefan and I are teaching this long version of\nhugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'",
            "metadata": {
              "position": 60,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:50:39.550",
              "token_count": 469
            },
            "id": "dc2ab3b5-3c39-47c3-82c8-761bf3a86db6",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:48:38.250]\nto your Point\nhugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n\nhugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n\nAnd to once again\nhugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n\nhugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n\nhugo bowne-anderson: You would get a huge amount out of this. I hope right?\n\nhugo bowne-anderson: Of course.\n\nhugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2",
            "metadata": {
              "position": 59,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:48:38.250",
              "token_count": 464
            },
            "id": "8c4209b4-f48b-44d7-8441-900f0f5131fe",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 8134.107828140259,
    "openai_duration_ms": 4312,
    "gemini_tokens": 2598,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864771"
  },
  {
    "id": "q11",
    "question": "What advice did Hugo give about integrating LLMs into existing workflows?",
    "user_type": "ml_engineer",
    "scenario": "factual",
    "gemini_response": [
      "Based on the provided transcript sections, Hugo discusses the lifecycle for building and deploying generative AI and LLM-powered apps, contrasting it with traditional software development. He mentions building involves prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied. He also describes a gradual rollout pattern for LLM applications (POC -> internal testing -> limited deployment -> full production).\n\nHowever, the sections do not contain specific advice from Hugo on *how* to integrate LLMs into *existing* workflows.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n"
    ],
    "openai_response": [
      "Hugo advised that integrating LLMs into existing workflows should be done gradually and iteratively. He mentioned a pattern for rolling out LLM applications, which includes starting with a proof of concept, conducting a risk assessment and cost evaluation, performing controlled testing, deploying as an internal tool to a limited number of users, setting up monitoring and evaluation systems, gathering feedback, and gradually expanding to full production. He emphasized the importance of iterating on the product and mentioned that deploying LLM-powered applications is not a binary process, but rather a continuum with multiple dimensions.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2549,
        "completion_tokens": 112,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:51:30.100]\nm going to say is that\nhugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\nStefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\nStefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\nStefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n\nhugo bowne-anderson: Exactly and the other thing, I'll just add,",
            "metadata": {
              "position": 23,
              "source": "workshop_transcript",
              "speaker": "Nathan",
              "timestamp": "00:51:30.100",
              "token_count": 489
            },
            "id": "8dc7ac65-2121-4ac9-b751-dfcb8f18299f",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 10137.42208480835,
    "openai_duration_ms": 2985,
    "gemini_tokens": 2580,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864773"
  },
  {
    "id": "q12",
    "question": "How does observability play a role in LLM application development?",
    "user_type": "data_scientist",
    "scenario": "technical",
    "gemini_response": [
      "Based on the provided transcript sections:\n\nObservability plays a role after the initial deployment of LLM-powered apps. Due to the non-deterministic nature of these applications (meaning their behavior isn't always predictable), monitoring and evaluation are crucial.\n\nObservability involves:\n\n1.  **Looking at traces:** To understand the flow and interactions within the app.\n2.  **General observability:** Gaining insight into the app and the conversations taking place.\n3.  **Visibility into internals:** Being able to introspect what's happening at each point, especially in complex setups like multi-turn conversations or agentic systems.\n4.  **Identifying failure modes:** By examining the data captured through monitoring, developers can see where the application is failing.\n5.  **Capturing data:** Logging, monitoring, and tracing are highlighted as a core principle, emphasizing the need to capture data.\n\nSetting up monitoring and evaluation systems is also mentioned as a specific step in the typical rollout process for LLM applications in industry.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "openai_response": [
      "Observability plays a role in LLM application development by allowing developers to monitor and trace the behavior of the application. It involves capturing data, logging, and monitoring the application to understand how it is performing and behaving. This helps in identifying failure modes, evaluating the application at both individual and business levels, and making improvements to the application.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2497,
        "completion_tokens": 68,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 9021.804094314575,
    "openai_duration_ms": 2644,
    "gemini_tokens": 2630,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864774"
  },
  {
    "id": "q13",
    "question": "What are the key considerations for production deployment of LLM applications?",
    "user_type": "ml_engineer",
    "scenario": "general",
    "gemini_response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 15\n}\n]",
      ""
    ],
    "openai_response": [
      "The key considerations for production deployment of LLM applications include:\n- Building production-grade systems that scale beyond proof of concepts\n- Iteration and hands-on development\n- Logging, monitoring, and tracing to capture data\n- Evaluating at both individual and business levels\n- Gradual rollout and iterative improvement\n- Working across multiple platforms and adopting tools mindfully",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2497,
        "completion_tokens": 72,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 2014.2388343811035,
    "openai_duration_ms": 2156,
    "gemini_tokens": 2428,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864776"
  },
  {
    "id": "q14",
    "question": "How should I handle errors and edge cases in LLM applications?",
    "user_type": "student",
    "scenario": "technical",
    "gemini_response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 9\n}\n]",
      ""
    ],
    "openai_response": [
      "In the provided sections of the workshop transcript, there is no specific information about how to handle errors and edge cases in LLM applications.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n4. [Distance: 1.0000] [Chunk 55] \n    [TIMESTAMP: 01:41:36.280]\n    But to see that play out on one profile is\n    hugo bowne-anderson: evidence that increased specificity can increase correct results if it's in the same direction. Now, you'd want to test that out with a lot of different profiles. And Pratishka, who's who's at Google working on Youtube in in particular, saw the same thing,\n    \n    that increasing specificity will\n    hugo bowne-anderson: of of the question of the prompt. And you can think of it as it. Because we have some probabilistic system, where the more we say, the more we narrow down the probability space of potential responses.\n    \n    hugo bowne-anderson: And but we want to make sure we're molding it in the right direction. The other thing that came up from Mika Castellani is hallucinations, and the fact that he's kept saying things that weren't from the document. And we spoke about this.\n    \n    But the\n    hugo bowne-anderson: what is it drawing on?\n    \n    hugo bowne-anderson: The context that you're giving it? Or is it drawing from the pre-trained weights of the Llm. And figuring out how to navigate these these challenges as well? So we saw a bunch of failure, modes and ways to improve and iterate, and we'll see more of that later later this week. I do want to ask one question, which is, I think, people in my group didn't quite realize this, and I don't know how to ask the question. So I'm going to just make the statement.\n    \n    hugo bowne-anderson: Actually, no, the question is, what prompt when you.\n    \n    hugo bowne-anderson: when you ask a question in this app, what prompt is sent to the Llm.\n    \n    hugo bowne-anderson: I'm going to tell you my answer.\n    \n    hugo bowne-anderson: I don't know.\n    \n    hugo bowne-anderson: I have no idea what was sent to my Llm. This is,\n    \n    and I apologize already for\n    hugo bowne-anderson: cussing. Oh, you can't see my screen yet, can you?\n    Stefan Krawczyk CEO DAGWorks Inc.: You're not sharing. Yeah.\n    \n    hugo bowne-anderson:\n\n5. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2530,
        "completion_tokens": 28,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:41:36.280]\nBut to see that play out on one profile is\nhugo bowne-anderson: evidence that increased specificity can increase correct results if it's in the same direction. Now, you'd want to test that out with a lot of different profiles. And Pratishka, who's who's at Google working on Youtube in in particular, saw the same thing,\n\nthat increasing specificity will\nhugo bowne-anderson: of of the question of the prompt. And you can think of it as it. Because we have some probabilistic system, where the more we say, the more we narrow down the probability space of potential responses.\n\nhugo bowne-anderson: And but we want to make sure we're molding it in the right direction. The other thing that came up from Mika Castellani is hallucinations, and the fact that he's kept saying things that weren't from the document. And we spoke about this.\n\nBut the\nhugo bowne-anderson: what is it drawing on?\n\nhugo bowne-anderson: The context that you're giving it? Or is it drawing from the pre-trained weights of the Llm. And figuring out how to navigate these these challenges as well? So we saw a bunch of failure, modes and ways to improve and iterate, and we'll see more of that later later this week. I do want to ask one question, which is, I think, people in my group didn't quite realize this, and I don't know how to ask the question. So I'm going to just make the statement.\n\nhugo bowne-anderson: Actually, no, the question is, what prompt when you.\n\nhugo bowne-anderson: when you ask a question in this app, what prompt is sent to the Llm.\n\nhugo bowne-anderson: I'm going to tell you my answer.\n\nhugo bowne-anderson: I don't know.\n\nhugo bowne-anderson: I have no idea what was sent to my Llm. This is,\n\nand I apologize already for\nhugo bowne-anderson: cussing. Oh, you can't see my screen yet, can you?\nStefan Krawczyk CEO DAGWorks Inc.: You're not sharing. Yeah.\n\nhugo bowne-anderson:",
            "metadata": {
              "position": 55,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "01:41:36.280",
              "token_count": 483
            },
            "id": "ede413b4-89bd-494d-8fa8-b241353cc965",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:51:30.100]\nm going to say is that\nhugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\nStefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\nStefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\nStefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n\nhugo bowne-anderson: Exactly and the other thing, I'll just add,",
            "metadata": {
              "position": 23,
              "source": "workshop_transcript",
              "speaker": "Nathan",
              "timestamp": "00:51:30.100",
              "token_count": 489
            },
            "id": "8dc7ac65-2121-4ac9-b751-dfcb8f18299f",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 1854.4797897338867,
    "openai_duration_ms": 3315,
    "gemini_tokens": 2461,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864778"
  },
  {
    "id": "q15",
    "question": "What did the workshop say about the importance of looking at your data?",
    "user_type": "data_scientist",
    "scenario": "factual",
    "gemini_response": [
      "Based on the workshop transcript sections provided:\n\nThe transcript emphasizes the importance of looking at your data, particularly in the context of generative AI and LLM-powered apps, due to their non-deterministic nature.\n\n*   **Monitoring and Evaluation:** It's stated that after deploying these apps, you must monitor and evaluate them because \"We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next\" [00:23:32.990].\n*   **Observability and Introspection:** Monitoring involves looking at traces and having general observability. You also need visibility into the internals, especially for complex setups like multi-turn conversations or agentic systems, to \"introspect into what's happening at each point\" [00:23:32.990].\n*   **Identifying Failure Modes:** The transcript explicitly states, \"once you start looking at your data there, you'll see where the failure modes are right\" [00:23:32.990]. It later reiterates this point regarding multi-turn conversations or multi-agents: \"If you put things in spreadsheets and look at them, you will see the failure modes\" [01:29:11.080].\n*   **Manual Annotation and Workflow:** A workflow is described where LLM responses are put into spreadsheets, annotated daily by a domain expert as correct or incorrect, and errors are highlighted. This manual inspection is deemed \"incredibly important before getting to the step of automating\" [01:29:11.080].\n*   **Validating Generated Data/Prompts:** Even when using LLMs or combinatorial methods to generate data or prompts synthetically, the transcript advises getting \"yourself in the loop\" or the \"domain expert in the loop\" to review and ensure the generated items are sensible and appropriate, comparing unintelligent use to following Google Maps down a one-way street [01:23:59.200].",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 47] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:29:11.080]\n    hugo bowne-anderson: Wrong?\n    \n    hugo bowne-anderson: Okay, you get the picture. And then maybe there was one which is.\n    \n    hugo bowne-anderson: yeah,\n    \n    hugo bowne-anderson: different to response,\n    \n    one\n    hugo bowne-anderson: which was the same question. So you can see now. And one of the reasons I want to work in a sheet and not in pandas is the ability to annotate this stuff. And, in fact.\n    \n    hugo bowne-anderson: for most of us a golden rule of working with spreadsheets is never to highlight rows. But if there's something that's really wrong,\n    \n    you should probably like\n    hugo bowne-anderson: this isn't even extracted as a list. Right? So Whoa.\n    \n    hugo bowne-anderson: yeah, I want to like, make that red. So I can share that with friends or colleagues and be like, Hey, we need to work on this. Okay?\n    \n    hugo bowne-anderson: Similarly, I will tell. It was Philip who I mentioned earlier from Honeycomb.\n    \n    I actually hung out with him at Kubecon EU a lot\n    hugo bowne-anderson: last year, and I was going to say earlier this year, but it was last year in Paris,\n    \n    and he told me that what what Hamill actually got him to do was\n    hugo bowne-anderson: almost daily work in spreadsheets where they got Llm. Responses, he would annotate them, say, what's correct and wrong, then put it through another Llm. With his responses, then turn that into an Llm. As judge, and he would daily, as the domain expert, get in spreadsheets and say, correct, incorrect, correct, incorrect, and they developed a system where they would flow like like that. Right?\n    \n    I think these types of workflows are incredibly important before getting to the step of automating right and\n    hugo bowne-anderson: to my point earlier. If someone, if any of you was like, how do I figure out how to evaluate my multiturn conversations or my multi agents. If you put things in spreadsheets and look at them, you will see the failure modes like you. If if you say, extract it as a list, and it never extracts it as a list.\n\n2. [Distance: 1.0000] [Chunk 54] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:40:17.490]\n    hugo bowne-anderson: so Gp has a good question checking the understanding.\n    \n    If you develop using code spaces and deploy using gradio that link is publicly available on the\n    hugo bowne-anderson: Internet,\n    Stefan Krawczyk CEO DAGWorks Inc.: You want. I mean, we can check that. Does anyone wanna.\n    \n    hugo bowne-anderson: Yeah. Why don't I share mine? Yeah, I.\n    Stefan Krawczyk CEO DAGWorks Inc.: Oh, yeah.\n    \n    hugo bowne-anderson: I think it is. Yeah, so I wouldn't share it with anyone.\n    Stefan Krawczyk CEO DAGWorks Inc.: If you click that you should see. Yeah, you should be able to see my app. But this is where in codespace. Obviously, you can.\n    Stefan Krawczyk CEO DAGWorks Inc.: cancel, or at least rather kill the process. And then, yeah.\n    \n    hugo bowne-anderson: Exactly.\n    Stefan Krawczyk CEO DAGWorks Inc.: 4, 4. Okay, no. So it knows. Local to me. Okay.\n    \n    hugo bowne-anderson: Oh, interesting!\n    Stefan Krawczyk CEO DAGWorks Inc.: Okay. Okay.\n    \n    hugo bowne-anderson: So we do need to move on. I did want to say the group I I chilled with so many fascinating things. 2\n    \n    takeaways were\n    hugo bowne-anderson: increase specificity. So if you ask actually, Yuan, would you mind giving your example.\n    \n    Yuan Liu: Sure, it's just a very high level example. For example, I just use Stephen's resume in the Pdf format and upload it. If I just ask, say what the Pdf. Is about. It just will give me a random answer that is not relevant, however, though if I add some prefix, say you are an AI assistant, and you are only allowed to answer, based on the provided information, and then attach your question. After that the answer is actually what we are looking for.\n    \n    hugo bowne-anderson: Exactly so increase special.\n    \n    And this is\n    hugo bowne-anderson: this is a reasonable assumption.\n\n3. [Distance: 1.0000] [Chunk 30] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:02:22.870]\n    hugo bowne-anderson: also? What type of salary you want to command as well. We live in a pretty strange world, where you know certain niche skills that you can learn pick up relatively easily, which are gate kept in a number of ways, can, you know, really increase your salary? And everyone we need to think about think about those things.\n    \n    hugo bowne-anderson: I will also, actually, no, I'll leave it.\n    \n    hugo bowne-anderson: Leave it at that, because I've got so many things coming to my mind, and I do want to continue. I haven't looked at this yet I just opened this.\n    \n    Can people see\n    hugo bowne-anderson: a bunch of? I'm so sorry for showing pie charts in a data centric course as well. That seems like a big no, no. But hey, Google, we have some people from Google here. Can you? Can you issue a Pr into Google forms to not make pie charts, please.\n    \n    hugo bowne-anderson: But anyway, so this is really exciting. We've got some really nice slices of the pie here. Okay, so we've got 40% of people haven't built or deployed, l empowered apps. Yet 22%\n    \n    have\n    hugo bowne-anderson: experimented locally but not deployed, and 40% have built and deployed applications. This is fantastic. Now, what this means, though, is that tailoring, as we'll see tailoring the content to everyone.\n    \n    hugo bowne-anderson: won't be straightforward, due to the variance here, but we'll definitely do our best, and if we're going too slow or too fast, we've got little surveys. At the end of each workshop. We'll figure out how to align with everyone on your needs as students as well. Okay, so have you worked with gradio or streamlit or similar tools to build front end interfaces for AI or data apps. No, I haven't.\n    \n    hugo bowne-anderson: 35% tried them. 40, 20% of have built apps very cool.\n\n4. [Distance: 1.0000] [Chunk 43] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:23:59.200]\n    hugo bowne-anderson: Then I'm going to write the results to a Csv.\n    \n    hugo bowne-anderson: and then what I'm going to do is when I write the query, I'm going to generate 20 queries to loop. So please generate 20, but not a thousand. If you're using the Api key, I provided. What you'll see is that I'm taking these base queries and then adding stuff like, Please return it in. Json. Extract it as a list focus on recent jobs. So what you'll note is, I'm actually synthetically generating prompts in some way. Now, I'm not doing it quite using an Llm. Right? I'm doing it combinatorially with these. Having said that.\n    \n    hugo bowne-anderson: I generated this exercise by chatting about it with Claude and Chatgpt, and so I chatted with Llms. For a while, and then decided this and and got Chat gpt to generate the base queries, for example. So don't don't be scared of using Llms in your workflow, but use them smartly.\n    \n    hugo bowne-anderson: What I'm going to tell you is a very silly example. But years ago I was in a car with a family member driving,\n    \n    and\n    hugo bowne-anderson: they nearly drove down a 1-way street.\n    \n    hugo bowne-anderson: and I said to them, you nearly drove down a 1-way street, and they said, Google Maps told me to drive there.\n    \n    hugo bowne-anderson: And what I'm trying to illustrate is that we can use. That's that's an example of an unintelligent use of technology. And sure there are a lot of other things at play, being in like rush hour traffic all of these things. But if you start using technology intelligently, like, if this had suggested a base query.\n    \n    Tell me about\n    hugo bowne-anderson: their childhood. I would be like, Oh, that's silly, because we're looking at Linkedin profiles. Right? So getting synthetically generated. Data isn't necessarily bad, but you need to get yourself in the loop to make sure. That is what you would want it to do. Get the domain expert in the loop. Right? So all that having been said.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n"
    ],
    "openai_response": [
      "The workshop emphasized the importance of looking at your data to identify failure modes and evaluate the performance of your AI or data applications. It mentioned that by examining the data, you can see where the failure modes are and ensure that the application is behaving as desired.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 47] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:29:11.080]\n    hugo bowne-anderson: Wrong?\n    \n    hugo bowne-anderson: Okay, you get the picture. And then maybe there was one which is.\n    \n    hugo bowne-anderson: yeah,\n    \n    hugo bowne-anderson: different to response,\n    \n    one\n    hugo bowne-anderson: which was the same question. So you can see now. And one of the reasons I want to work in a sheet and not in pandas is the ability to annotate this stuff. And, in fact.\n    \n    hugo bowne-anderson: for most of us a golden rule of working with spreadsheets is never to highlight rows. But if there's something that's really wrong,\n    \n    you should probably like\n    hugo bowne-anderson: this isn't even extracted as a list. Right? So Whoa.\n    \n    hugo bowne-anderson: yeah, I want to like, make that red. So I can share that with friends or colleagues and be like, Hey, we need to work on this. Okay?\n    \n    hugo bowne-anderson: Similarly, I will tell. It was Philip who I mentioned earlier from Honeycomb.\n    \n    I actually hung out with him at Kubecon EU a lot\n    hugo bowne-anderson: last year, and I was going to say earlier this year, but it was last year in Paris,\n    \n    and he told me that what what Hamill actually got him to do was\n    hugo bowne-anderson: almost daily work in spreadsheets where they got Llm. Responses, he would annotate them, say, what's correct and wrong, then put it through another Llm. With his responses, then turn that into an Llm. As judge, and he would daily, as the domain expert, get in spreadsheets and say, correct, incorrect, correct, incorrect, and they developed a system where they would flow like like that. Right?\n    \n    I think these types of workflows are incredibly important before getting to the step of automating right and\n    hugo bowne-anderson: to my point earlier. If someone, if any of you was like, how do I figure out how to evaluate my multiturn conversations or my multi agents. If you put things in spreadsheets and look at them, you will see the failure modes like you. If if you say, extract it as a list, and it never extracts it as a list.\n\n2. [Distance: 1.0000] [Chunk 54] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:40:17.490]\n    hugo bowne-anderson: so Gp has a good question checking the understanding.\n    \n    If you develop using code spaces and deploy using gradio that link is publicly available on the\n    hugo bowne-anderson: Internet,\n    Stefan Krawczyk CEO DAGWorks Inc.: You want. I mean, we can check that. Does anyone wanna.\n    \n    hugo bowne-anderson: Yeah. Why don't I share mine? Yeah, I.\n    Stefan Krawczyk CEO DAGWorks Inc.: Oh, yeah.\n    \n    hugo bowne-anderson: I think it is. Yeah, so I wouldn't share it with anyone.\n    Stefan Krawczyk CEO DAGWorks Inc.: If you click that you should see. Yeah, you should be able to see my app. But this is where in codespace. Obviously, you can.\n    Stefan Krawczyk CEO DAGWorks Inc.: cancel, or at least rather kill the process. And then, yeah.\n    \n    hugo bowne-anderson: Exactly.\n    Stefan Krawczyk CEO DAGWorks Inc.: 4, 4. Okay, no. So it knows. Local to me. Okay.\n    \n    hugo bowne-anderson: Oh, interesting!\n    Stefan Krawczyk CEO DAGWorks Inc.: Okay. Okay.\n    \n    hugo bowne-anderson: So we do need to move on. I did want to say the group I I chilled with so many fascinating things. 2\n    \n    takeaways were\n    hugo bowne-anderson: increase specificity. So if you ask actually, Yuan, would you mind giving your example.\n    \n    Yuan Liu: Sure, it's just a very high level example. For example, I just use Stephen's resume in the Pdf format and upload it. If I just ask, say what the Pdf. Is about. It just will give me a random answer that is not relevant, however, though if I add some prefix, say you are an AI assistant, and you are only allowed to answer, based on the provided information, and then attach your question. After that the answer is actually what we are looking for.\n    \n    hugo bowne-anderson: Exactly so increase special.\n    \n    And this is\n    hugo bowne-anderson: this is a reasonable assumption.\n\n3. [Distance: 1.0000] [Chunk 30] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:02:22.870]\n    hugo bowne-anderson: also? What type of salary you want to command as well. We live in a pretty strange world, where you know certain niche skills that you can learn pick up relatively easily, which are gate kept in a number of ways, can, you know, really increase your salary? And everyone we need to think about think about those things.\n    \n    hugo bowne-anderson: I will also, actually, no, I'll leave it.\n    \n    hugo bowne-anderson: Leave it at that, because I've got so many things coming to my mind, and I do want to continue. I haven't looked at this yet I just opened this.\n    \n    Can people see\n    hugo bowne-anderson: a bunch of? I'm so sorry for showing pie charts in a data centric course as well. That seems like a big no, no. But hey, Google, we have some people from Google here. Can you? Can you issue a Pr into Google forms to not make pie charts, please.\n    \n    hugo bowne-anderson: But anyway, so this is really exciting. We've got some really nice slices of the pie here. Okay, so we've got 40% of people haven't built or deployed, l empowered apps. Yet 22%\n    \n    have\n    hugo bowne-anderson: experimented locally but not deployed, and 40% have built and deployed applications. This is fantastic. Now, what this means, though, is that tailoring, as we'll see tailoring the content to everyone.\n    \n    hugo bowne-anderson: won't be straightforward, due to the variance here, but we'll definitely do our best, and if we're going too slow or too fast, we've got little surveys. At the end of each workshop. We'll figure out how to align with everyone on your needs as students as well. Okay, so have you worked with gradio or streamlit or similar tools to build front end interfaces for AI or data apps. No, I haven't.\n    \n    hugo bowne-anderson: 35% tried them. 40, 20% of have built apps very cool.\n\n4. [Distance: 1.0000] [Chunk 43] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:23:59.200]\n    hugo bowne-anderson: Then I'm going to write the results to a Csv.\n    \n    hugo bowne-anderson: and then what I'm going to do is when I write the query, I'm going to generate 20 queries to loop. So please generate 20, but not a thousand. If you're using the Api key, I provided. What you'll see is that I'm taking these base queries and then adding stuff like, Please return it in. Json. Extract it as a list focus on recent jobs. So what you'll note is, I'm actually synthetically generating prompts in some way. Now, I'm not doing it quite using an Llm. Right? I'm doing it combinatorially with these. Having said that.\n    \n    hugo bowne-anderson: I generated this exercise by chatting about it with Claude and Chatgpt, and so I chatted with Llms. For a while, and then decided this and and got Chat gpt to generate the base queries, for example. So don't don't be scared of using Llms in your workflow, but use them smartly.\n    \n    hugo bowne-anderson: What I'm going to tell you is a very silly example. But years ago I was in a car with a family member driving,\n    \n    and\n    hugo bowne-anderson: they nearly drove down a 1-way street.\n    \n    hugo bowne-anderson: and I said to them, you nearly drove down a 1-way street, and they said, Google Maps told me to drive there.\n    \n    hugo bowne-anderson: And what I'm trying to illustrate is that we can use. That's that's an example of an unintelligent use of technology. And sure there are a lot of other things at play, being in like rush hour traffic all of these things. But if you start using technology intelligently, like, if this had suggested a base query.\n    \n    Tell me about\n    hugo bowne-anderson: their childhood. I would be like, Oh, that's silly, because we're looking at Linkedin profiles. Right? So getting synthetically generated. Data isn't necessarily bad, but you need to get yourself in the loop to make sure. That is what you would want it to do. Get the domain expert in the loop. Right? So all that having been said.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2564,
        "completion_tokens": 52,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 01:29:11.080]\nhugo bowne-anderson: Wrong?\n\nhugo bowne-anderson: Okay, you get the picture. And then maybe there was one which is.\n\nhugo bowne-anderson: yeah,\n\nhugo bowne-anderson: different to response,\n\none\nhugo bowne-anderson: which was the same question. So you can see now. And one of the reasons I want to work in a sheet and not in pandas is the ability to annotate this stuff. And, in fact.\n\nhugo bowne-anderson: for most of us a golden rule of working with spreadsheets is never to highlight rows. But if there's something that's really wrong,\n\nyou should probably like\nhugo bowne-anderson: this isn't even extracted as a list. Right? So Whoa.\n\nhugo bowne-anderson: yeah, I want to like, make that red. So I can share that with friends or colleagues and be like, Hey, we need to work on this. Okay?\n\nhugo bowne-anderson: Similarly, I will tell. It was Philip who I mentioned earlier from Honeycomb.\n\nI actually hung out with him at Kubecon EU a lot\nhugo bowne-anderson: last year, and I was going to say earlier this year, but it was last year in Paris,\n\nand he told me that what what Hamill actually got him to do was\nhugo bowne-anderson: almost daily work in spreadsheets where they got Llm. Responses, he would annotate them, say, what's correct and wrong, then put it through another Llm. With his responses, then turn that into an Llm. As judge, and he would daily, as the domain expert, get in spreadsheets and say, correct, incorrect, correct, incorrect, and they developed a system where they would flow like like that. Right?\n\nI think these types of workflows are incredibly important before getting to the step of automating right and\nhugo bowne-anderson: to my point earlier. If someone, if any of you was like, how do I figure out how to evaluate my multiturn conversations or my multi agents. If you put things in spreadsheets and look at them, you will see the failure modes like you. If if you say, extract it as a list, and it never extracts it as a list.",
            "metadata": {
              "position": 47,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:29:11.080",
              "token_count": 497
            },
            "id": "4b813594-f19f-4d71-ad5d-fc326af24a9d",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:40:17.490]\nhugo bowne-anderson: so Gp has a good question checking the understanding.\n\nIf you develop using code spaces and deploy using gradio that link is publicly available on the\nhugo bowne-anderson: Internet,\nStefan Krawczyk CEO DAGWorks Inc.: You want. I mean, we can check that. Does anyone wanna.\n\nhugo bowne-anderson: Yeah. Why don't I share mine? Yeah, I.\nStefan Krawczyk CEO DAGWorks Inc.: Oh, yeah.\n\nhugo bowne-anderson: I think it is. Yeah, so I wouldn't share it with anyone.\nStefan Krawczyk CEO DAGWorks Inc.: If you click that you should see. Yeah, you should be able to see my app. But this is where in codespace. Obviously, you can.\nStefan Krawczyk CEO DAGWorks Inc.: cancel, or at least rather kill the process. And then, yeah.\n\nhugo bowne-anderson: Exactly.\nStefan Krawczyk CEO DAGWorks Inc.: 4, 4. Okay, no. So it knows. Local to me. Okay.\n\nhugo bowne-anderson: Oh, interesting!\nStefan Krawczyk CEO DAGWorks Inc.: Okay. Okay.\n\nhugo bowne-anderson: So we do need to move on. I did want to say the group I I chilled with so many fascinating things. 2\n\ntakeaways were\nhugo bowne-anderson: increase specificity. So if you ask actually, Yuan, would you mind giving your example.\n\nYuan Liu: Sure, it's just a very high level example. For example, I just use Stephen's resume in the Pdf format and upload it. If I just ask, say what the Pdf. Is about. It just will give me a random answer that is not relevant, however, though if I add some prefix, say you are an AI assistant, and you are only allowed to answer, based on the provided information, and then attach your question. After that the answer is actually what we are looking for.\n\nhugo bowne-anderson: Exactly so increase special.\n\nAnd this is\nhugo bowne-anderson: this is a reasonable assumption.",
            "metadata": {
              "position": 54,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:40:17.490",
              "token_count": 478
            },
            "id": "0c1a1e85-a267-4dc8-a6f5-80467787db42",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:02:22.870]\nhugo bowne-anderson: also? What type of salary you want to command as well. We live in a pretty strange world, where you know certain niche skills that you can learn pick up relatively easily, which are gate kept in a number of ways, can, you know, really increase your salary? And everyone we need to think about think about those things.\n\nhugo bowne-anderson: I will also, actually, no, I'll leave it.\n\nhugo bowne-anderson: Leave it at that, because I've got so many things coming to my mind, and I do want to continue. I haven't looked at this yet I just opened this.\n\nCan people see\nhugo bowne-anderson: a bunch of? I'm so sorry for showing pie charts in a data centric course as well. That seems like a big no, no. But hey, Google, we have some people from Google here. Can you? Can you issue a Pr into Google forms to not make pie charts, please.\n\nhugo bowne-anderson: But anyway, so this is really exciting. We've got some really nice slices of the pie here. Okay, so we've got 40% of people haven't built or deployed, l empowered apps. Yet 22%\n\nhave\nhugo bowne-anderson: experimented locally but not deployed, and 40% have built and deployed applications. This is fantastic. Now, what this means, though, is that tailoring, as we'll see tailoring the content to everyone.\n\nhugo bowne-anderson: won't be straightforward, due to the variance here, but we'll definitely do our best, and if we're going too slow or too fast, we've got little surveys. At the end of each workshop. We'll figure out how to align with everyone on your needs as students as well. Okay, so have you worked with gradio or streamlit or similar tools to build front end interfaces for AI or data apps. No, I haven't.\n\nhugo bowne-anderson: 35% tried them. 40, 20% of have built apps very cool.",
            "metadata": {
              "position": 30,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:02:22.870",
              "token_count": 450
            },
            "id": "faa5a754-7454-4995-893f-76059459e5d8",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:23:59.200]\nhugo bowne-anderson: Then I'm going to write the results to a Csv.\n\nhugo bowne-anderson: and then what I'm going to do is when I write the query, I'm going to generate 20 queries to loop. So please generate 20, but not a thousand. If you're using the Api key, I provided. What you'll see is that I'm taking these base queries and then adding stuff like, Please return it in. Json. Extract it as a list focus on recent jobs. So what you'll note is, I'm actually synthetically generating prompts in some way. Now, I'm not doing it quite using an Llm. Right? I'm doing it combinatorially with these. Having said that.\n\nhugo bowne-anderson: I generated this exercise by chatting about it with Claude and Chatgpt, and so I chatted with Llms. For a while, and then decided this and and got Chat gpt to generate the base queries, for example. So don't don't be scared of using Llms in your workflow, but use them smartly.\n\nhugo bowne-anderson: What I'm going to tell you is a very silly example. But years ago I was in a car with a family member driving,\n\nand\nhugo bowne-anderson: they nearly drove down a 1-way street.\n\nhugo bowne-anderson: and I said to them, you nearly drove down a 1-way street, and they said, Google Maps told me to drive there.\n\nhugo bowne-anderson: And what I'm trying to illustrate is that we can use. That's that's an example of an unintelligent use of technology. And sure there are a lot of other things at play, being in like rush hour traffic all of these things. But if you start using technology intelligently, like, if this had suggested a base query.\n\nTell me about\nhugo bowne-anderson: their childhood. I would be like, Oh, that's silly, because we're looking at Linkedin profiles. Right? So getting synthetically generated. Data isn't necessarily bad, but you need to get yourself in the loop to make sure. That is what you would want it to do. Get the domain expert in the loop. Right? So all that having been said.",
            "metadata": {
              "position": 43,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:23:59.200",
              "token_count": 492
            },
            "id": "513bb452-001c-422a-bd95-5199dd8cb5e2",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 12198.583126068115,
    "openai_duration_ms": 1819,
    "gemini_tokens": 2896,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864779"
  },
  {
    "id": "q16",
    "question": "How can I measure the success of my LLM application?",
    "user_type": "ml_engineer",
    "scenario": "general",
    "gemini_response": [
      "Based on the transcript sections provided:\n\nYou can measure the success of your LLM application on multiple levels:\n\n1.  **Monitoring and Observability:** Set up monitoring to look at traces, have general observability into your app and conversations, and visibility into the internals, especially for complex systems. This helps identify failure modes.\n2.  **Individual Call Evaluation:** Evaluate at an individual level to ensure the LLM call itself is correct (e.g., extracting the correct information from a source).\n3.  **Output Quality Evaluation:** Assess the quality of the generated output. This might involve domain experts (e.g., having an HR person review emails generated by a recruiter app).\n4.  **Business Level Evaluation:** Ultimately, measure success by determining if the application is serving the business goal (e.g., for a recruiter app, is it helping recruit more efficiently and get better candidates?).\n5.  **User Feedback:** Gather qualitative user feedback during deployment phases.\n6.  **Performance Metrics:** Look at performance metrics during rollout.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "openai_response": [
      "To measure the success of an LLM application, you can evaluate it at both an individual level and a business level. At the individual level, you can assess whether the LLM call is correct and if it generates the desired output. For example, in a recruiter app that automates emails based on LinkedIn profiles, you can check if the app extracts the correct information from the profiles.\n\nAt the business level, you want to ensure that the LLM application is serving the overall goals of the business. In the recruiter app example, the business goal is to recruit more efficiently and get better candidates. So, you would evaluate if the app is achieving these objectives.\n\nIt's important to consider both individual and business-level evaluations to measure the success of an LLM application.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2496,
        "completion_tokens": 154,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 10553.632020950317,
    "openai_duration_ms": 3582,
    "gemini_tokens": 2636,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864781"
  },
  {
    "id": "q17",
    "question": "What was discussed about API parameters and their effects on LLM outputs?",
    "user_type": "student",
    "scenario": "technical",
    "gemini_response": [
      "Based on the provided transcript sections, there is no discussion about specific API parameters or their effects on LLM outputs. The sections mention making API calls, using vendor APIs, and the non-deterministic nature of LLMs (same input can lead to different outputs), but they do not go into detail about parameters like temperature, top_p, etc., or how they influence the results.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n5. [Distance: 1.0000] [Chunk 36] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:11:47.200]\n    hugo bowne-anderson: you'll see this. Okay, so we'\n    \n    re going to navigate to workshop one\n    hugo bowne-anderson: into the Apps directory and what you'll see is that there's a file here in workshops, in.\n    \n    hugo bowne-anderson: in workshops. One apps called Dot, N. So that's where we want to put our Api keys. I've got an Openai Api key here, and I've got a Google Gemini key as well. Stefan, would you mind getting a thumbs up and thumbs down from people of whether they have their own open. AI Api key.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, go to your chat on your own open AI,\n    Stefan Krawczyk CEO DAGWorks Inc.: so go to the chat and you react thumbs up thumbs down to my question, or give us a sense whether you have it.\n    \n    hugo bowne-anderson: And for those that do not have Openai Api keys.\n    \n    Shame on Openai for\n    hugo bowne-anderson: not having a free tier to be honest, because wouldn't that be wonderful if they let everyone just play around with it briefly,\n    \n    but I will give you\n    hugo bowne-anderson: okay. I'm gonna put like 10 bucks on it or something.\n    \n    So\n    hugo bowne-anderson: just know this is a shared resource. And if you want to use it more, please get your own Openai Api key. You can also use Google Gemini, which we will in a second.\n    \n    hugo bowne-anderson: and it has a free tier as well. But this one I'll share in a second. What am I naming it?\n    \n    hugo bowne-anderson: lm, sdlc.\n    \n    hugo bowne-anderson: so all I'm saying is, please don't tax this one a huge amount, so I'm going to show an example where we can do a for loop to ping the Api a bunch of times in order to see non-determinism at play. Please do not do that for loop a thousand times, for example.\n    \n    hugo bowne-anderson: So I'm putting the Api key. They're getting longer and longer, aren't they?\n\n"
    ],
    "openai_response": [
      "In the provided sections, there is no specific discussion about API parameters and their effects on LLM outputs.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n5. [Distance: 1.0000] [Chunk 36] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:11:47.200]\n    hugo bowne-anderson: you'll see this. Okay, so we'\n    \n    re going to navigate to workshop one\n    hugo bowne-anderson: into the Apps directory and what you'll see is that there's a file here in workshops, in.\n    \n    hugo bowne-anderson: in workshops. One apps called Dot, N. So that's where we want to put our Api keys. I've got an Openai Api key here, and I've got a Google Gemini key as well. Stefan, would you mind getting a thumbs up and thumbs down from people of whether they have their own open. AI Api key.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, go to your chat on your own open AI,\n    Stefan Krawczyk CEO DAGWorks Inc.: so go to the chat and you react thumbs up thumbs down to my question, or give us a sense whether you have it.\n    \n    hugo bowne-anderson: And for those that do not have Openai Api keys.\n    \n    Shame on Openai for\n    hugo bowne-anderson: not having a free tier to be honest, because wouldn't that be wonderful if they let everyone just play around with it briefly,\n    \n    but I will give you\n    hugo bowne-anderson: okay. I'm gonna put like 10 bucks on it or something.\n    \n    So\n    hugo bowne-anderson: just know this is a shared resource. And if you want to use it more, please get your own Openai Api key. You can also use Google Gemini, which we will in a second.\n    \n    hugo bowne-anderson: and it has a free tier as well. But this one I'll share in a second. What am I naming it?\n    \n    hugo bowne-anderson: lm, sdlc.\n    \n    hugo bowne-anderson: so all I'm saying is, please don't tax this one a huge amount, so I'm going to show an example where we can do a for loop to ping the Api a bunch of times in order to see non-determinism at play. Please do not do that for loop a thousand times, for example.\n    \n    hugo bowne-anderson: So I'm putting the Api key. They're getting longer and longer, aren't they?\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2503,
        "completion_tokens": 22,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:51:30.100]\nm going to say is that\nhugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\nStefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\nStefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\nStefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n\nhugo bowne-anderson: Exactly and the other thing, I'll just add,",
            "metadata": {
              "position": 23,
              "source": "workshop_transcript",
              "speaker": "Nathan",
              "timestamp": "00:51:30.100",
              "token_count": 489
            },
            "id": "8dc7ac65-2121-4ac9-b751-dfcb8f18299f",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:11:47.200]\nhugo bowne-anderson: you'll see this. Okay, so we'\n\nre going to navigate to workshop one\nhugo bowne-anderson: into the Apps directory and what you'll see is that there's a file here in workshops, in.\n\nhugo bowne-anderson: in workshops. One apps called Dot, N. So that's where we want to put our Api keys. I've got an Openai Api key here, and I've got a Google Gemini key as well. Stefan, would you mind getting a thumbs up and thumbs down from people of whether they have their own open. AI Api key.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah, go to your chat on your own open AI,\nStefan Krawczyk CEO DAGWorks Inc.: so go to the chat and you react thumbs up thumbs down to my question, or give us a sense whether you have it.\n\nhugo bowne-anderson: And for those that do not have Openai Api keys.\n\nShame on Openai for\nhugo bowne-anderson: not having a free tier to be honest, because wouldn't that be wonderful if they let everyone just play around with it briefly,\n\nbut I will give you\nhugo bowne-anderson: okay. I'm gonna put like 10 bucks on it or something.\n\nSo\nhugo bowne-anderson: just know this is a shared resource. And if you want to use it more, please get your own Openai Api key. You can also use Google Gemini, which we will in a second.\n\nhugo bowne-anderson: and it has a free tier as well. But this one I'll share in a second. What am I naming it?\n\nhugo bowne-anderson: lm, sdlc.\n\nhugo bowne-anderson: so all I'm saying is, please don't tax this one a huge amount, so I'm going to show an example where we can do a for loop to ping the Api a bunch of times in order to see non-determinism at play. Please do not do that for loop a thousand times, for example.\n\nhugo bowne-anderson: So I'm putting the Api key. They're getting longer and longer, aren't they?",
            "metadata": {
              "position": 36,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:11:47.200",
              "token_count": 480
            },
            "id": "4d54466c-f545-44c5-bdf4-71e2482231a4",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 9477.596282958984,
    "openai_duration_ms": 3014,
    "gemini_tokens": 2511,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864783"
  },
  {
    "id": "q18",
    "question": "What ethical considerations should I keep in mind when building LLM applications?",
    "user_type": "data_scientist",
    "scenario": "general",
    "gemini_response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]",
      ""
    ],
    "openai_response": [
      "The provided sections of the workshop transcript do not mention anything about ethical considerations when building LLM applications.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2479,
        "completion_tokens": 21,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 1815.9003257751465,
    "openai_duration_ms": 1823,
    "gemini_tokens": 2409,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864784"
  },
  {
    "id": "q19",
    "question": "How did the first workshop address the challenge of hallucinations in LLMs?",
    "user_type": "ml_engineer",
    "scenario": "technical",
    "gemini_response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 9\n}\n]",
      ""
    ],
    "openai_response": [
      "The first workshop addressed the challenge of hallucinations in LLMs by discussing the issue and acknowledging that it can occur. The instructor mentioned that one of the participants experienced hallucinations in their LLM and they discussed the possible reasons for it. They also mentioned the importance of understanding the context and prompt given to the LLM and how it can affect the generated responses. However, the workshop did not provide specific solutions or strategies to address hallucinations in LLMs.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n3. [Distance: 1.0000] [Chunk 55] \n    [TIMESTAMP: 01:41:36.280]\n    But to see that play out on one profile is\n    hugo bowne-anderson: evidence that increased specificity can increase correct results if it's in the same direction. Now, you'd want to test that out with a lot of different profiles. And Pratishka, who's who's at Google working on Youtube in in particular, saw the same thing,\n    \n    that increasing specificity will\n    hugo bowne-anderson: of of the question of the prompt. And you can think of it as it. Because we have some probabilistic system, where the more we say, the more we narrow down the probability space of potential responses.\n    \n    hugo bowne-anderson: And but we want to make sure we're molding it in the right direction. The other thing that came up from Mika Castellani is hallucinations, and the fact that he's kept saying things that weren't from the document. And we spoke about this.\n    \n    But the\n    hugo bowne-anderson: what is it drawing on?\n    \n    hugo bowne-anderson: The context that you're giving it? Or is it drawing from the pre-trained weights of the Llm. And figuring out how to navigate these these challenges as well? So we saw a bunch of failure, modes and ways to improve and iterate, and we'll see more of that later later this week. I do want to ask one question, which is, I think, people in my group didn't quite realize this, and I don't know how to ask the question. So I'm going to just make the statement.\n    \n    hugo bowne-anderson: Actually, no, the question is, what prompt when you.\n    \n    hugo bowne-anderson: when you ask a question in this app, what prompt is sent to the Llm.\n    \n    hugo bowne-anderson: I'm going to tell you my answer.\n    \n    hugo bowne-anderson: I don't know.\n    \n    hugo bowne-anderson: I have no idea what was sent to my Llm. This is,\n    \n    and I apologize already for\n    hugo bowne-anderson: cussing. Oh, you can't see my screen yet, can you?\n    Stefan Krawczyk CEO DAGWorks Inc.: You're not sharing. Yeah.\n    \n    hugo bowne-anderson:\n\n4. [Distance: 1.0000] [Chunk 8] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:20:49.162]\n    hugo bowne-anderson: Thanks so much for that context. And, Stefan, I love it. And I was going to go through the syllabus, but I think showing you some of this.\n    \n    A few slides that I wanted to\n    hugo bowne-anderson: show will be very instructive here. So with respect to the 1st principles, thinking so, you may have seen that we think about it as Llms are wonderful, but you almost immediately get in this proof of concept purgatory right where,\n    \n    if you think about traditional software development\n    hugo bowne-anderson: and think about excitement as a function of time. The scientist in me doesn't even know what units to put on the axis of excitement. But we'll skip that for the time being. But you start off with something small, a Hello world! Which isn't that exciting? Then you add basic features, bit more excitement.\n    \n    hugo bowne-anderson: Add unit tests. Oh, this is looking cool. Then you start scaling and optimizing. Then you load balance. And suddenly you're getting more and more exciting.\n    \n    hugo bowne-anderson: Now what's happening here with Llms is immediately you get a flashy demo. Then you get basic functionality. Then you're like, Oh, wait!\n    \n    hugo bowne-anderson: I got some hallucinations, and your excitement goes down. Then you've got some monitoring challenges, and then you've got integration issues. So excitement goes down pretty quickly as a function of time. So we don't. Oh, pardon me.\n    \n    hugo bowne-anderson: I'm getting ahead of myself. Whoa! How do I even get back?\n    \n    hugo bowne-anderson: We don't want to move this per se. We just want to shift all of this stuff up. And the truth is, the flashy Demos are easy work shifting this stuff up is the hard work, and that's what we're here to do. So, as I said before, when thinking about the traditional software development lifecycle, you know, like this is reductive.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2491,
        "completion_tokens": 94,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:41:36.280]\nBut to see that play out on one profile is\nhugo bowne-anderson: evidence that increased specificity can increase correct results if it's in the same direction. Now, you'd want to test that out with a lot of different profiles. And Pratishka, who's who's at Google working on Youtube in in particular, saw the same thing,\n\nthat increasing specificity will\nhugo bowne-anderson: of of the question of the prompt. And you can think of it as it. Because we have some probabilistic system, where the more we say, the more we narrow down the probability space of potential responses.\n\nhugo bowne-anderson: And but we want to make sure we're molding it in the right direction. The other thing that came up from Mika Castellani is hallucinations, and the fact that he's kept saying things that weren't from the document. And we spoke about this.\n\nBut the\nhugo bowne-anderson: what is it drawing on?\n\nhugo bowne-anderson: The context that you're giving it? Or is it drawing from the pre-trained weights of the Llm. And figuring out how to navigate these these challenges as well? So we saw a bunch of failure, modes and ways to improve and iterate, and we'll see more of that later later this week. I do want to ask one question, which is, I think, people in my group didn't quite realize this, and I don't know how to ask the question. So I'm going to just make the statement.\n\nhugo bowne-anderson: Actually, no, the question is, what prompt when you.\n\nhugo bowne-anderson: when you ask a question in this app, what prompt is sent to the Llm.\n\nhugo bowne-anderson: I'm going to tell you my answer.\n\nhugo bowne-anderson: I don't know.\n\nhugo bowne-anderson: I have no idea what was sent to my Llm. This is,\n\nand I apologize already for\nhugo bowne-anderson: cussing. Oh, you can't see my screen yet, can you?\nStefan Krawczyk CEO DAGWorks Inc.: You're not sharing. Yeah.\n\nhugo bowne-anderson:",
            "metadata": {
              "position": 55,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "01:41:36.280",
              "token_count": 483
            },
            "id": "ede413b4-89bd-494d-8fa8-b241353cc965",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:20:49.162]\nhugo bowne-anderson: Thanks so much for that context. And, Stefan, I love it. And I was going to go through the syllabus, but I think showing you some of this.\n\nA few slides that I wanted to\nhugo bowne-anderson: show will be very instructive here. So with respect to the 1st principles, thinking so, you may have seen that we think about it as Llms are wonderful, but you almost immediately get in this proof of concept purgatory right where,\n\nif you think about traditional software development\nhugo bowne-anderson: and think about excitement as a function of time. The scientist in me doesn't even know what units to put on the axis of excitement. But we'll skip that for the time being. But you start off with something small, a Hello world! Which isn't that exciting? Then you add basic features, bit more excitement.\n\nhugo bowne-anderson: Add unit tests. Oh, this is looking cool. Then you start scaling and optimizing. Then you load balance. And suddenly you're getting more and more exciting.\n\nhugo bowne-anderson: Now what's happening here with Llms is immediately you get a flashy demo. Then you get basic functionality. Then you're like, Oh, wait!\n\nhugo bowne-anderson: I got some hallucinations, and your excitement goes down. Then you've got some monitoring challenges, and then you've got integration issues. So excitement goes down pretty quickly as a function of time. So we don't. Oh, pardon me.\n\nhugo bowne-anderson: I'm getting ahead of myself. Whoa! How do I even get back?\n\nhugo bowne-anderson: We don't want to move this per se. We just want to shift all of this stuff up. And the truth is, the flashy Demos are easy work shifting this stuff up is the hard work, and that's what we're here to do. So, as I said before, when thinking about the traditional software development lifecycle, you know, like this is reductive.",
            "metadata": {
              "position": 8,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:20:49.162",
              "token_count": 443
            },
            "id": "85b5d155-b706-4e48-8f95-f454c4a0a34b",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 799.5350360870361,
    "openai_duration_ms": 2274,
    "gemini_tokens": 2419,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864786"
  },
  {
    "id": "q2",
    "question": "Who were the instructors for the first workshop?",
    "user_type": "student",
    "scenario": "factual",
    "gemini_response": [
      "Based on the transcript sections provided, the instructors are:\n\n1.  Hugo Bowne-Anderson\n2.  Stefan Krawczyk",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n4. [Distance: 1.0000] [Chunk 5] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:13:17.160]\n    and when we go through the syllabus\n    hugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n    \n    and\n    hugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n    \n    I think him and Philip\n    hugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n    \n    which is now like\n    hugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n    \n    Kumar from\n    hugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n    \n    hugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n    \n    The other thing is\n    hugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "openai_response": [
      "The instructors for the first workshop were Hugo Bowne-Anderson and Stefan Krawczyk.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n4. [Distance: 1.0000] [Chunk 5] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:13:17.160]\n    and when we go through the syllabus\n    hugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n    \n    and\n    hugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n    \n    I think him and Philip\n    hugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n    \n    which is now like\n    hugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n    \n    Kumar from\n    hugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n    \n    hugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n    \n    The other thing is\n    hugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2482,
        "completion_tokens": 21,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 01:50:39.550]\nh\nhugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n\nWhat I\nhugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\nStefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n\nhugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n\nhugo bowne-anderson: Everything we need in the modal form.\n\nhugo bowne-anderson: And I'll do that next time.\n\nAnd then\nhugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n\nst time\nhugo bowne-anderson:\n\nStefan and I are teaching this long version of\nhugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'",
            "metadata": {
              "position": 60,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:50:39.550",
              "token_count": 469
            },
            "id": "dc2ab3b5-3c39-47c3-82c8-761bf3a86db6",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:02:17.182]\nhugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\nStefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\nStefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\nStefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\nStefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\nStefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\nStefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?",
            "metadata": {
              "position": 1,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:02:17.182",
              "token_count": 454
            },
            "id": "3353de3a-52fe-4aa6-8573-9794d60dbdd2",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:13:17.160]\nand when we go through the syllabus\nhugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n\nand\nhugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n\nI think him and Philip\nhugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n\nwhich is now like\nhugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n\nKumar from\nhugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n\nhugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n\nThe other thing is\nhugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,",
            "metadata": {
              "position": 5,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:13:17.160",
              "token_count": 469
            },
            "id": "083aaf1e-ee26-4839-800d-b56641223102",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 7274.947881698608,
    "openai_duration_ms": 2424,
    "gemini_tokens": 2446,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864756"
  },
  {
    "id": "q20",
    "question": "What examples or case studies were presented in the first workshop?",
    "user_type": "student",
    "scenario": "factual",
    "gemini_response": [
      "Based on the provided transcript sections, the main example built during the first workshop session was a foundational app for **querying PDFs and generating responses**.\n\nThe transcript mentions that in the first week, they would:\n*   Set up this foundational app ([TIMESTAMP: 00:43:32.890]).\n*   Build the MVP (Minimum Viable Product) of this app ([TIMESTAMP: 01:48:38.250]).\n*   Iterate on prompts for this app ([TIMESTAMP: 01:48:38.250]).\n*   Add interactivity using Gradio ([TIMESTAMP: 01:48:38.250]).\n*   Start logging and visualizing logs related to this app ([TIMESTAMP: 01:48:38.250]).\n\nNo other specific examples or case studies were described as being presented in the first session within these transcript sections.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n3. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n"
    ],
    "openai_response": [
      "Based on the provided sections of the workshop transcript, there is no specific mention of examples or case studies that were presented in the first workshop.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n3. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2546,
        "completion_tokens": 29,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:03:25.470]\nhugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n\nhugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n\nsetting up evals for Llm apps great\nhugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n\nhugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n\nwhich is\nhugo bowne-anderson: super exciting.\n\nSo\nhugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n\nm going to\nhugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n\nWould you mind\nhugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n\nhugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\nStefan Krawczyk CEO DAGWorks Inc.: And.",
            "metadata": {
              "position": 31,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:03:25.470",
              "token_count": 486
            },
            "id": "079ab785-e4ff-468c-a82c-6cd9022be1a2",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:48:38.250]\nto your Point\nhugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n\nhugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n\nAnd to once again\nhugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n\nhugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n\nhugo bowne-anderson: You would get a huge amount out of this. I hope right?\n\nhugo bowne-anderson: Of course.\n\nhugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2",
            "metadata": {
              "position": 59,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:48:38.250",
              "token_count": 464
            },
            "id": "8c4209b4-f48b-44d7-8441-900f0f5131fe",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:00:00.290]\nhugo bowne-anderson: Everyone.\n\nhugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n\nso\nhugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n\nBut in the meantime\nhugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n\nhugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n\nand previously I was in\nhugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n\nWhat I mean by that is\nhugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n\nhugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n\nhugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.",
            "metadata": {
              "position": 0,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:00:00.290",
              "token_count": 485
            },
            "id": "4a117ebe-0532-43df-8a26-587946cb93e4",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 14491.96720123291,
    "openai_duration_ms": 2344,
    "gemini_tokens": 2656,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864787"
  },
  {
    "id": "q3",
    "question": "What is the software development lifecycle for LLM applications?",
    "user_type": "ml_engineer",
    "scenario": "general",
    "gemini_response": [
      "Based on the transcript sections provided:\n\nThe software development lifecycle for generative AI and LLM powered apps is described as follows:\n\n1.  **Specs:** Define the specifications.\n2.  **Build:** This can involve prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied.\n3.  **Deploy Quickly:** Deployment should consider unit tests, continuous integration, and versioning.\n4.  **Monitor:** Due to the non-deterministic nature of LLMs, monitoring is crucial. This involves looking at traces, having general observability into the app and conversations, and visibility into the internals (especially for complex systems).\n5.  **Evaluate:** Evaluation occurs at both an individual level (e.g., is the LLM call correct?) and a business level (e.g., is the app achieving the intended business goal?).\n\nThe transcript emphasizes that monitoring and evaluation are particularly important because the behavior of LLM applications isn't always predictable (\"We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next\"). This cycle allows for iteration based on observed performance and evaluation results.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "openai_response": [
      "The software development lifecycle for LLM applications involves the following steps:\n\n1. Proof of concept\n2. Risk assessment and cost evaluation\n3. Controlled testing and deployment as an internal tool\n4. Monitoring setup and evaluation systems\n5. Gathering feedback\n6. Limited deployment to a small percentage of users\n7. Performance metrics and user feedback evaluation\n8. Gradual expansion to full production\n\nIt is important to note that deploying LLM applications is not a binary process, but rather a gradual rollout with continuous iteration and improvement.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2495,
        "completion_tokens": 108,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 10667.594909667969,
    "openai_duration_ms": 2323,
    "gemini_tokens": 2663,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864759"
  },
  {
    "id": "q4",
    "question": "What frameworks or tools were demonstrated in the first workshop?",
    "user_type": "data_scientist",
    "scenario": "factual",
    "gemini_response": [
      "Based on the transcript sections provided:\n\n*   **Llama Index**\n*   **Gradio**\n*   **SQLite**\n*   **Dataset** (a tool by Salem and Willison)\n\nThe speaker also mentions they \"may even get to modal for deployment\" and will use \"one or 2 vendor Apis\" [00:15:16.300], and later summarizes that they added interactivity with Gradio and started logging/visualizing logs [01:48:38.250]. They also walked through spinning up a **Codespace** [01:03:25.470].",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n3. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n"
    ],
    "openai_response": [
      "In the first workshop, the following frameworks and tools were mentioned or demonstrated:\n- Discord for communication and networking\n- LAMA Index for querying PDFs and generating responses\n- Gradio for adding interactivity to the app\n- SQLite for logging and visualizing logs\n- Dataset, a tool by Simon Willison, for data management\n- Modal for hardware for LLM developers",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n3. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2540,
        "completion_tokens": 77,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:48:38.250]\nto your Point\nhugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n\nhugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n\nAnd to once again\nhugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n\nhugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n\nhugo bowne-anderson: You would get a huge amount out of this. I hope right?\n\nhugo bowne-anderson: Of course.\n\nhugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2",
            "metadata": {
              "position": 59,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:48:38.250",
              "token_count": 464
            },
            "id": "8c4209b4-f48b-44d7-8441-900f0f5131fe",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 01:03:25.470]\nhugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n\nhugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n\nsetting up evals for Llm apps great\nhugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n\nhugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n\nwhich is\nhugo bowne-anderson: super exciting.\n\nSo\nhugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n\nm going to\nhugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n\nWould you mind\nhugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n\nhugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\nStefan Krawczyk CEO DAGWorks Inc.: And.",
            "metadata": {
              "position": 31,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "01:03:25.470",
              "token_count": 486
            },
            "id": "079ab785-e4ff-468c-a82c-6cd9022be1a2",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:18:01.480]\nSo\nhugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n\nnot to cover every scaling challenge in every technology\nhugo bowne-anderson: challenge. Okay,\n\nhugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n\nhugo bowne-anderson: I mean,\n\nif we were to teach you\nhugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n\nSo\nhugo bowne-anderson: no.\nStefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\nStefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\nStefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.",
            "metadata": {
              "position": 7,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:18:01.480",
              "token_count": 480
            },
            "id": "a7787505-1b3a-4014-b20b-838b4fbae891",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 15478.111028671265,
    "openai_duration_ms": 2685,
    "gemini_tokens": 2593,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864761"
  },
  {
    "id": "q5",
    "question": "How can I approach prompt engineering according to Workshop 1?",
    "user_type": "student",
    "scenario": "technical",
    "gemini_response": [
      "Based on the provided transcript sections:\n\nPrompt engineering is presented as a key part of the principle \"iterate quickly\" in the Llm software development lifecycle. The approach involves:\n\n1.  **Logging inputs and outputs:** To understand what the system is doing.\n2.  **Evaluating logs:** Analyzing the logged data to identify issues like non-determinism or poor performance.\n3.  **Iterating on prompts and context:** Modifying the prompts and the context provided to the Llm based on the evaluation to improve results and reduce variance.\n\nThis iterative process, which includes prompt engineering alongside other techniques like fine-tuning and changing business logic, is part of building a software development lifecycle that allows for rapid adjustments.\n\nThe transcript also mentions that prompt engineering will be covered extensively in the second session of week 1 and the first session of week 2, including a guest lecture specifically on prompt engineering in the Llm software development lifecycle. Advanced prompt optimization is also mentioned as a potential topic for week 4.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 20] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:47:10.640]\n    hugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n    \n    hugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n    \n    And\n    hugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n    \n    hugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n    \n    s actually going to be on\n    hugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n    \n    But a recording will be\n    hugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n    \n    hugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.\n\n3. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n4. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n5. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n"
    ],
    "openai_response": [
      "According to Workshop 1, prompt engineering can be approached by logging inputs and outputs, evaluating logs, iterating on prompts and context, and using API knobs to reduce variance. It is also important to leverage domain expertise to evaluate the output in development and production. Additionally, involving the person who used to write the emails (in the case of generating emails) can help in evaluating the quality of the output.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 20] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:47:10.640]\n    hugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n    \n    hugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n    \n    And\n    hugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n    \n    hugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n    \n    s actually going to be on\n    hugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n    \n    But a recording will be\n    hugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n    \n    hugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.\n\n3. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n4. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n5. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2507,
        "completion_tokens": 81,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:47:10.640]\nhugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n\nhugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n\nAnd\nhugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n\nhugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n\ns actually going to be on\nhugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n\nBut a recording will be\nhugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n\nhugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.",
            "metadata": {
              "position": 20,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:47:10.640",
              "token_count": 480
            },
            "id": "e0b75b76-369f-4987-aa01-27fff71a4979",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:18:01.480]\nSo\nhugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n\nnot to cover every scaling challenge in every technology\nhugo bowne-anderson: challenge. Okay,\n\nhugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n\nhugo bowne-anderson: I mean,\n\nif we were to teach you\nhugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n\nSo\nhugo bowne-anderson: no.\nStefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\nStefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\nStefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.",
            "metadata": {
              "position": 7,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:18:01.480",
              "token_count": 480
            },
            "id": "a7787505-1b3a-4014-b20b-838b4fbae891",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:27:43.690]\nhugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n\nre going to use llama index and bur and all these other things at a variety\nhugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n\nhugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n\nGet them in the loop\nhugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n\nhugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.",
            "metadata": {
              "position": 11,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:27:43.690",
              "token_count": 452
            },
            "id": "886564c1-688b-4090-8d94-46004ddc053d",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:02:17.182]\nhugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\nStefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\nStefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\nStefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\nStefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\nStefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\nStefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\nStefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?",
            "metadata": {
              "position": 1,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:02:17.182",
              "token_count": 454
            },
            "id": "3353de3a-52fe-4aa6-8573-9794d60dbdd2",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 8409.33609008789,
    "openai_duration_ms": 2734,
    "gemini_tokens": 2643,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864763"
  },
  {
    "id": "q6",
    "question": "What are the stages of building an LLM application discussed in the workshop?",
    "user_type": "ml_engineer",
    "scenario": "general",
    "gemini_response": [
      "Based on the transcript sections provided:\n\nThe workshop discusses the lifecycle for generative AI and LLM powered apps as follows:\n\n1.  **Specs:** Defining the requirements.\n2.  **Build:** This can involve prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied.\n3.  **Deploy quickly:** Getting the application out.\n4.  **Monitor and Evaluate:** This is crucial due to the non-deterministic nature of LLMs. It involves looking at traces, general observability, introspection into internals (especially for multi-turn or agentic systems), and evaluating at both individual call and business levels.\n\nAdditionally, a pattern for rolling out LLM applications is mentioned:\n\n1.  **Proof of concept (POC)**\n2.  **Risk assessment & cost evaluation**\n3.  **Controlled testing** (deploying as an internal tool)\n4.  **Monitoring setup & evaluation systems**\n5.  **Gather feedback**\n6.  **Limited deployment** (e.g., to 0.1% or 1% of users)\n7.  **Look more at performance metrics & get user feedback**\n8.  **Gradual expansion to full production**",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n"
    ],
    "openai_response": [
      "The stages of building an LLM application discussed in the workshop are as follows:\n\n1. Proof of concept\n2. Risk assessment and cost evaluation\n3. Controlled testing and deployment as an internal tool\n4. Monitoring setup and evaluation systems\n5. Gathering feedback\n6. Limited deployment to a small percentage of users\n7. Performance metrics and user feedback evaluation\n8. Gradual expansion to full production",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2549,
        "completion_tokens": 83,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 8412.701845169067,
    "openai_duration_ms": 1769,
    "gemini_tokens": 2727,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864764"
  },
  {
    "id": "q7",
    "question": "What did Hugo say about evaluation and iteration in LLM applications?",
    "user_type": "data_scientist",
    "scenario": "factual",
    "gemini_response": [
      "Based on the transcript sections provided:\n\nHugo described evaluation and iteration as key parts of the lifecycle for generative AI and LLM-powered apps, particularly after the deployment stage due to their non-deterministic nature.\n\n**Regarding Evaluation:**\n\n*   It follows the \"deploy quickly\" stage because you don't know exactly how the LLM will behave.\n*   It involves looking at data and results to quantify performance.\n*   It requires monitoring, logging, tracing, and general observability into the app and conversations.\n*   Evaluation should happen at multiple levels:\n    *   **Individual level:** Checking if the LLM call is correct (e.g., extracting the right information from a profile).\n    *   **Output quality level:** Assessing if the generated output is good (e.g., does an automated email look like one a human expert would write?). This often requires domain expertise (like getting an HR person to evaluate recruitment emails).\n    *   **Business level:** Determining if the application is serving the overall business goal (e.g., is the recruiter app actually helping recruit more efficiently and get better candidates?). Aligning outputs with business metrics is crucial.\n*   It's listed as a \"1st principle\" for the course.\n*   It's essential for tackling challenges like non-determinism and hallucinations, often involving leveraging domain expertise.\n*   Setting up evaluation systems is part of the controlled rollout process for LLM applications.\n\n**Regarding Iteration:**\n\n*   It follows the monitoring and evaluation phases.\n*   It involves making changes quickly based on evaluation findings.\n*   Methods include prompt engineering, fine-tuning, and changing business logic.\n*   It's listed as a \"1st principle\" for the course (\"Iterate quickly\").\n*   It's part of the strategy for handling non-determinism (iterating on prompts and context).\n*   The software development lifecycle for LLMs should be built to allow for quick iteration.\n*   Iteration happens concurrently with the gradual rollout of an LLM application.\n*   Week 2 of the workshop is dedicated to iteration, evaluation, and observability.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n"
    ],
    "openai_response": [
      "Hugo mentioned that evaluation and iteration are important in LLM applications. He emphasized the need to evaluate at both the individual level (e.g., evaluating if the LLM call is correct) and the business level (e.g., evaluating if the LLM is serving the business goals). He also mentioned the importance of iterating quickly, which involves prompt engineering, fine tuning, and changing business logic.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2498,
        "completion_tokens": 80,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:27:43.690]\nhugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n\nre going to use llama index and bur and all these other things at a variety\nhugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n\nhugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n\nGet them in the loop\nhugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n\nhugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.",
            "metadata": {
              "position": 11,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:27:43.690",
              "token_count": 452
            },
            "id": "886564c1-688b-4090-8d94-46004ddc053d",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 14348.545789718628,
    "openai_duration_ms": 3190,
    "gemini_tokens": 2860,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864766"
  },
  {
    "id": "q8",
    "question": "What is the difference between LLM fine-tuning and prompt engineering according to the workshop?",
    "user_type": "student",
    "scenario": "technical",
    "gemini_response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 22\n}\n]",
      ""
    ],
    "openai_response": [
      "According to the workshop, LLM fine-tuning and prompt engineering are both important aspects of LLM software development. Fine-tuning involves adjusting the pre-trained language model to make it more suitable for a specific task or domain. On the other hand, prompt engineering focuses on designing effective prompts or instructions to elicit the desired responses from the language model. Both fine-tuning and prompt engineering contribute to improving the performance and usability of LLM-powered applications.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 20] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:47:10.640]\n    hugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n    \n    hugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n    \n    And\n    hugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n    \n    hugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n    \n    s actually going to be on\n    hugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n    \n    But a recording will be\n    hugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n    \n    hugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.\n\n3. [Distance: 1.0000] [Chunk 5] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:13:17.160]\n    and when we go through the syllabus\n    hugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n    \n    and\n    hugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n    \n    I think him and Philip\n    hugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n    \n    which is now like\n    hugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n    \n    Kumar from\n    hugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n    \n    hugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n    \n    The other thing is\n    hugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2556,
        "completion_tokens": 91,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:43:32.890]\nhugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n\nhugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n\nre going to introduce you to Genai and Llm development\nhugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n\nBased applications and set up the foundational app which is querying Pdfs and generating responses\nhugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n\nhugo bowne-anderson: We have several guest lectures, as you're aware.\n\nhugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n\ns just\nhugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n\nand the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\nhugo bowne-anderson: Week 3, we'",
            "metadata": {
              "position": 18,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:43:32.890",
              "token_count": 488
            },
            "id": "368ddbc6-0858-4493-87a9-b2406ca0acd7",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:47:10.640]\nhugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n\nhugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n\nAnd\nhugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n\nhugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n\ns actually going to be on\nhugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n\nBut a recording will be\nhugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n\nhugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.",
            "metadata": {
              "position": 20,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:47:10.640",
              "token_count": 480
            },
            "id": "e0b75b76-369f-4987-aa01-27fff71a4979",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:13:17.160]\nand when we go through the syllabus\nhugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n\nand\nhugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n\nI think him and Philip\nhugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n\nwhich is now like\nhugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n\nKumar from\nhugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n\nhugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n\nThe other thing is\nhugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,",
            "metadata": {
              "position": 5,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:13:17.160",
              "token_count": 469
            },
            "id": "083aaf1e-ee26-4839-800d-b56641223102",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 1361.0920906066895,
    "openai_duration_ms": 1991,
    "gemini_tokens": 2482,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864768"
  },
  {
    "id": "q9",
    "question": "How should I think about data when building LLM applications?",
    "user_type": "data_scientist",
    "scenario": "general",
    "gemini_response": [
      "Based on the transcript sections provided:\n\nYou should think about data in the context of LLM applications primarily through the lens of monitoring, evaluation, and understanding the application's behavior, especially given its non-deterministic nature.\n\nHere's a breakdown:\n\n1.  **Capture Data:** A core principle mentioned is \"logging, monitoring, and tracing,\" stating explicitly, \"You need to capture data.\" This includes inputs and outputs of LLM calls, traces, and conversations happening within the app.\n2.  **Monitor and Observe:** Because LLMs are non-deterministic and their behavior isn't always predictable, you need to monitor them after deployment. This involves looking at traces and having general observability into the app and its conversations. You need visibility into the internals to understand what's happening at each point.\n3.  **Identify Failure Modes:** By \"looking at your data\" gathered through monitoring, you can identify where failure modes are occurring.\n4.  **Evaluate:** Data is essential for evaluation at multiple levels:\n    *   **Individual Level:** Checking if an individual LLM call is correct (e.g., extracting the right information).\n    *   **Business Level:** Assessing if the application is achieving its intended business goals (e.g., recruiting more efficiently), not just generating plausible outputs.\n5.  **Gather Feedback:** During rollout phases (like limited deployment), you should gather qualitative user feedback and look at performance metrics, which are forms of data used for iteration.\n6.  **Input/Output:** At a fundamental level, interacting with LLMs involves API calls with input and output, which is a form of data interaction.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "openai_response": [
      "When building LLM applications, it is important to think about data in several ways. First, LLM applications involve pinging LLMs with API calls and considering the input and output of these calls. Second, LLMs are non-deterministic, meaning that they can produce different outputs for the same inputs. This is different from traditional software. Third, logging, monitoring, and tracing are crucial for capturing data and understanding the behavior of LLM applications. Finally, evaluation should be done at both the individual level (evaluating the correctness of LLM calls) and the business level (evaluating if the application is serving the intended business goals).",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n",
      {
        "num_chunks": 5,
        "context_tokens": 2477,
        "completion_tokens": 132,
        "embedding_tokens": 7680,
        "chunks": [
          {
            "text": "[TIMESTAMP: 00:23:32.990]\nBut I think\nhugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n\nThat type of thing\nhugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n\nSo building can involve\nhugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n\nyou want to look at traces and have general observability into\nhugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n\nand\nhugo bowne-anderson:\n\nyou also want to have\nhugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n\nhugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n\nmaking sure that\nhugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.",
            "metadata": {
              "position": 9,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:23:32.990",
              "token_count": 490
            },
            "id": "b71f55ee-37cb-4ec8-825a-f768cdce9984",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:25:14.560]\nhugo bowne-anderson: Then the next level is,\n\ndoes it generate an email that\nhugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n\nhugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n\nhugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n\nhugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n\nThis is not software\nhugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.",
            "metadata": {
              "position": 10,
              "source": "workshop_transcript",
              "speaker": "Unknown",
              "timestamp": "00:25:14.560",
              "token_count": 419
            },
            "id": "d19ca652-3107-498b-9bec-54d1040b6eba",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:29:34.600]\nRecently they published a database\nhugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n\nI do\nhugo bowne-anderson: want to.\n\nhugo bowne-anderson: It is fascinating because you can, right.\n\nhugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n\nBut\nhugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n\nhugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n\nhugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n\nSo I just want to make clear that\nhugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.",
            "metadata": {
              "position": 12,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:29:34.600",
              "token_count": 494
            },
            "id": "dcf5453b-844c-4ba7-8a10-7c4ccb708a62",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:15:16.300]\nand people\nhugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n\nhugo bowne-anderson: So I think that's something we can get out of this course as well.\n\nSo\nhugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n\nBut the focus will not\nhugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n\nd be happy to discuss this\nhugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n\nSo\nhugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.",
            "metadata": {
              "position": 6,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:15:16.300",
              "token_count": 469
            },
            "id": "4fe6116f-073f-4dfe-a05c-2aac9d745f67",
            "relevance": 1.0
          },
          {
            "text": "[TIMESTAMP: 00:09:56.800]\nt\nhugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n\nhugo bowne-anderson: so please do. Message me your github handle and we'\n\nll get started on that in\nhugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n\nhugo bowne-anderson:\n\nSo\nhugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n\nMachine learners quote unquote\nhugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n\nhugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n\nhugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,",
            "metadata": {
              "position": 4,
              "source": "workshop_transcript",
              "speaker": "hugo bowne-anderson",
              "timestamp": "00:09:56.800",
              "token_count": 451
            },
            "id": "21a208b2-c96b-4066-8ea0-328a91b22808",
            "relevance": 1.0
          }
        ]
      }
    ],
    "gemini_duration_ms": 12116.174936294556,
    "openai_duration_ms": 2235,
    "gemini_tokens": 2745,
    "openai_tokens": 0,
    "timestamp": "2025-03-28T23:09:31.864769"
  }
]