[
  {
    "question": "What are the first principles for building LLM applications mentioned in Workshop 1?",
    "response": [
      "Based on the provided transcript sections, the first principles mentioned for building LLM applications are:\n\n1.  **API calls (Input/Output):** LLMs involve API calls where you send input and receive output.\n2.  **Non-determinism:** LLMs can produce different outputs even with the same input.\n3.  **Logging, monitoring, and tracing:** You need to capture data related to the LLM's operation.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n2. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n"
    ],
    "duration_ms": 8191.991090774536,
    "timestamp": "2025-03-28T23:00:56.781972",
    "model_provider": "gemini",
    "context_tokens": 2406,
    "completion_tokens": 90,
    "id": "q1",
    "user_type": "student",
    "scenario": "general",
    "judgment": "fail",
    "reason": "The new system response is incomplete and lacks several key first principles mentioned in the workshop. While it correctly identifies non-determinism and the importance of logging, monitoring, and tracing, it misses other critical principles such as focusing on workflows, building production-grade systems, and the practical approach emphasized in the workshop. Additionally, the mention of API calls is not framed as a first principle in the context of the workshop. The response does not provide a comprehensive or accurate representation of the first principles discussed.",
    "evaluation_type": "llm"
  },
  {
    "question": "Who were the instructors for the first workshop?",
    "response": [
      "Based on the transcript sections provided, the instructors are:\n\n1.  Hugo Bowne-Anderson\n2.  Stefan Krawczyk",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n4. [Distance: 1.0000] [Chunk 5] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:13:17.160]\n    and when we go through the syllabus\n    hugo bowne-anderson: we'll see this play out. But we're going to have guest lectures from people really on the cutting edge. So I suppose I talked about the cutting edge of tech companies. There are also a handful of people I know who are on the cutting edge,\n    \n    and\n    hugo bowne-anderson: in some ways further ahead the curve of adoption and technology than myself and Stefan not necessarily settled technology. So this isn't stuff I'd take out and tell everyone to adopt immediately. But people such as Sean Swicks working with agents. Hamel Hussein's work on Llms in production is exceptional.\n    \n    I think him and Philip\n    hugo bowne-anderson: at Honeycomb they actually outside the vendor Apis. They were the 1st to put an Llm. In production. Essentially, it was text to, not SQL. But text to honeycomb query, language. Sander Schulhoff, who published,\n    \n    which is now like\n    hugo bowne-anderson: it's called the prompt report. But essentially it's a report written by 40 people about prompt engineering people from Openai Sand, a bunch of others coming and talking about prompt engineering Charles from Modal, where we've got a thousand dollars of credits for each of you to talk about hardware and Ravan, Kumar, Ravin,\n    \n    Kumar from\n    hugo bowne-anderson: the AI labs at Google, where he worked on notebook. Lm, and now, mariner, and he's going to come and give a talk which I'm working on with him currently about end-to-end products. So how to design user experiences from machine learning models. And his work at AI labs.\n    \n    hugo bowne-anderson: Google, AI is going to inform a lot of lot of that. So I just want to make sure that you don't only get kind of the base foundational stuff. But you see what people are doing on the cutting edge as well.\n    \n    The other thing is\n    hugo bowne-anderson: these types of courses. Of course Stefan and I are here to teach and offer as much value as as we can and and beyond. But sometimes these courses have a magic of their own,\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "duration_ms": 7274.947881698608,
    "timestamp": "2025-03-28T23:01:09.062248",
    "model_provider": "gemini",
    "context_tokens": 2417,
    "completion_tokens": 29,
    "id": "q2",
    "user_type": "student",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question by providing the names of the instructors for the first workshop, Hugo Bowne-Anderson and Stefan Krawczyk. It is specific and factually correct based on the workshop content, and it avoids any hallucinations or made-up information.",
    "evaluation_type": "llm"
  },
  {
    "question": "What is the software development lifecycle for LLM applications?",
    "response": [
      "Based on the transcript sections provided:\n\nThe software development lifecycle for generative AI and LLM powered apps is described as follows:\n\n1.  **Specs:** Define the specifications.\n2.  **Build:** This can involve prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied.\n3.  **Deploy Quickly:** Deployment should consider unit tests, continuous integration, and versioning.\n4.  **Monitor:** Due to the non-deterministic nature of LLMs, monitoring is crucial. This involves looking at traces, having general observability into the app and conversations, and visibility into the internals (especially for complex systems).\n5.  **Evaluate:** Evaluation occurs at both an individual level (e.g., is the LLM call correct?) and a business level (e.g., is the app achieving the intended business goal?).\n\nThe transcript emphasizes that monitoring and evaluation are particularly important because the behavior of LLM applications isn't always predictable (\"We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next\"). This cycle allows for iteration based on observed performance and evaluation results.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n2. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n3. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "duration_ms": 10667.594909667969,
    "timestamp": "2025-03-28T23:01:24.731511",
    "model_provider": "gemini",
    "context_tokens": 2428,
    "completion_tokens": 235,
    "id": "q3",
    "user_type": "ml_engineer",
    "scenario": "general",
    "judgment": "pass",
    "reason": "The response directly answers the question by outlining a specific software development lifecycle for LLM applications, including stages such as defining specifications, building, deploying, monitoring, and evaluating. It is factually correct based on the workshop content and emphasizes the importance of monitoring and evaluation due to the non-deterministic nature of LLMs. The response avoids hallucinations and provides a clear, structured answer that aligns with the workshop's teachings.",
    "evaluation_type": "llm"
  },
  {
    "question": "What frameworks or tools were demonstrated in the first workshop?",
    "response": [
      "Based on the transcript sections provided:\n\n*   **Llama Index**\n*   **Gradio**\n*   **SQLite**\n*   **Dataset** (a tool by Salem and Willison)\n\nThe speaker also mentions they \"may even get to modal for deployment\" and will use \"one or 2 vendor Apis\" [00:15:16.300], and later summarizes that they added interactivity with Gradio and started logging/visualizing logs [01:48:38.250]. They also walked through spinning up a **Codespace** [01:03:25.470].",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n3. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n"
    ],
    "duration_ms": 15478.111028671265,
    "timestamp": "2025-03-28T23:01:45.213421",
    "model_provider": "gemini",
    "context_tokens": 2472,
    "completion_tokens": 121,
    "id": "q4",
    "user_type": "data_scientist",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question by listing specific frameworks and tools demonstrated in the first workshop, such as Llama Index, Gradio, SQLite, and Dataset. It also provides additional context about the use of Gradio for interactivity and mentions the potential use of modal for deployment and vendor APIs. The response is factually correct based on the provided transcript sections and avoids any hallucinations or fabricated content.",
    "evaluation_type": "llm"
  },
  {
    "question": "How can I approach prompt engineering according to Workshop 1?",
    "response": [
      "Based on the provided transcript sections:\n\nPrompt engineering is presented as a key part of the principle \"iterate quickly\" in the Llm software development lifecycle. The approach involves:\n\n1.  **Logging inputs and outputs:** To understand what the system is doing.\n2.  **Evaluating logs:** Analyzing the logged data to identify issues like non-determinism or poor performance.\n3.  **Iterating on prompts and context:** Modifying the prompts and the context provided to the Llm based on the evaluation to improve results and reduce variance.\n\nThis iterative process, which includes prompt engineering alongside other techniques like fine-tuning and changing business logic, is part of building a software development lifecycle that allows for rapid adjustments.\n\nThe transcript also mentions that prompt engineering will be covered extensively in the second session of week 1 and the first session of week 2, including a guest lecture specifically on prompt engineering in the Llm software development lifecycle. Advanced prompt optimization is also mentioned as a potential topic for week 4.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 20] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:47:10.640]\n    hugo bowne-anderson: then in week 4, we're gonna jump into from customization to deployment. So there are a lot of different paths. We can go down. And this is what we'll figure out with you what you want to learn the most from advanced prompt optimization to fine tuning, to preparing data sets synthetically generating data, sets multi agent workflows talking through future trends. So over the next couple of weeks we'll get a sense of what you what would help you the most to learn out of these things.\n    \n    hugo bowne-anderson: and we'll be building more and more guest lectures that week will be Hamel Hussein, who's a good friend of ours, and he's going to give a talk on basic data literacy for debugging and evaluating Llms. He's been running office hours for the past year where people come with Llm. Application problems. And they'll say I've got these like multi agent multi-turn conversations and these complex rag pipelines.\n    \n    And\n    hugo bowne-anderson: I need to evaluate them. And he'll say, just look at your data. And he actually says people still don't understand what he means by that.\n    \n    hugo bowne-anderson: And people may not have the data literacy skills to. As I was saying before, jump into, look at the traces internally within an agentic pipeline, and you'll see the failure modes emerge. And we'll see examples of this today. And actually, so I'm really excited for this session. It will be outside class time. It'll be on. Oh, this is hilarious. It'\n    \n    s actually going to be on\n    hugo bowne-anderson: Tuesday, Tuesday. At this time I wrote Wednesday because it is Wednesday for me again.\n    \n    But a recording will be\n    hugo bowne-anderson: available, and we may even release it as a podcast so you'll have access to that session whether you can make it or not. And Sean swicks from the latent space podcast and small AI, AI engineer will be talking about engineering, AI agents in 2025.\n    \n    hugo bowne-anderson: Eric has a great question. I heard that AI. Agents is the future. Is this a reasonable statement? I will answer that. And this is something we'll go through.\n\n3. [Distance: 1.0000] [Chunk 7] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:18:01.480]\n    So\n    hugo bowne-anderson: if you came for like a plug and play AI recipe book, that's not what this course is, but you will learn how to iterate, debug, and adapt, not just follow templates if you're looking for like if you came here and like, I want to learn like how to hyperscale and load, balance a rag and deal with large context, for you know a thousand users. That is not this course, either. This course, is designed to get you moving in the right direction with all those things,\n    \n    not to cover every scaling challenge in every technology\n    hugo bowne-anderson: challenge. Okay,\n    \n    hugo bowne-anderson: in a lot of ways, and we'll now go through the high level schedule. But you'll appreciate in 8 2\u00a0h sessions.\n    \n    hugo bowne-anderson: I mean,\n    \n    if we were to teach you\n    hugo bowne-anderson: how to, you know, develop production, ready load, balanced hyperscalable rag systems. That's an 8 week. Sorry. That's an 8 more than a 16\u00a0h course in itself. So we're going for principles and breadth rather than very serious depth into into everything.\n    \n    So\n    hugo bowne-anderson: no.\n    Stefan Krawczyk CEO DAGWorks Inc.: So, so just to kind of answer that I mean so in terms of moving in the in the right direction. So what we mean by 1st principles is really, you know, the tools and tech are gonna be changing. But really, we're trying to like\n    Stefan Krawczyk CEO DAGWorks Inc.: empower you and teach you the kind of the skills where you'll be able to take that thinking. And basically, any framework, or any tool that comes after or before. After this, rather, or even the ones kind of. Now you'll have a better appreciation and potentially understanding. For whether a it's the right tool for you even. But then be yeah, like.\n    Stefan Krawczyk CEO DAGWorks Inc.: If, as the tools change, you know you, you should, you should feel comfortable. That yeah, the skills that you're learning will empower you to kind of build stuff.\n\n4. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n5. [Distance: 1.0000] [Chunk 1] \n    [TIMESTAMP: 00:02:17.182]\n    hugo bowne-anderson: Looking at data and figuring out how you evaluate your systems. Incredibly important. So that's a brief introduction to me touching on what's in in the course as well, Stefan, maybe you can introduce yourself also.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah. Hi, Stefan Kafchuk.\n    Stefan Krawczyk CEO DAGWorks Inc.: so super excited to to be here in terms of my background. I grew up in New Zealand, being out here in the bay since 2,007. So I live in San Francisco\n    Stefan Krawczyk CEO DAGWorks Inc.: throughout my career. The only thing I haven't done is mobile development front end stuff, but effectively in the last kind of 10 years has been building on the data platform mlop side of things. And most recently, Gen. AI on the Lmops side of things.\n    Stefan Krawczyk CEO DAGWorks Inc.: Specifically, I have been driving 2 open source projects, Hamilton and Burr. I'll put links in the chat if you're curious, but effectively have been trying to think about, how do you enable software engineers and have them transition to AI engineers? And then, conversely, how do you help\n    Stefan Krawczyk CEO DAGWorks Inc.: the people on the data. Ml, side, take their skills and then apply them, build more software and product. Right? So very much interested in what is the new software development lifecycle shaping up to be since the agile sprint model doesn't really work when you're building something with Gen. AI as as it would as it did with traditional software, but otherwise, yeah, super excited for the amount of experience that's on here. I'm hopeful I can learn stuff from you guys as well\n    Stefan Krawczyk CEO DAGWorks Inc.: but otherwise yeah, I will be, you know. Q will be driving most of this this course. I'll be kind of helping where I can with things, but otherwise\n    Stefan Krawczyk CEO DAGWorks Inc.: Do you guys anything else to transition to? Otherwise?\n\n"
    ],
    "duration_ms": 8409.33609008789,
    "timestamp": "2025-03-28T23:01:58.627101",
    "model_provider": "gemini",
    "context_tokens": 2439,
    "completion_tokens": 204,
    "id": "q5",
    "user_type": "student",
    "scenario": "technical",
    "judgment": "pass",
    "reason": "The response directly answers the question by outlining specific details about the approach to prompt engineering as discussed in Workshop 1. It mentions the importance of logging inputs and outputs, evaluating logs, and iterating on prompts and context, which aligns with the workshop's emphasis on an iterative development process. The response is factually correct based on the provided transcript sections and avoids any hallucinations or fabricated content.",
    "evaluation_type": "llm"
  },
  {
    "question": "What are the stages of building an LLM application discussed in the workshop?",
    "response": [
      "Based on the transcript sections provided:\n\nThe workshop discusses the lifecycle for generative AI and LLM powered apps as follows:\n\n1.  **Specs:** Defining the requirements.\n2.  **Build:** This can involve prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied.\n3.  **Deploy quickly:** Getting the application out.\n4.  **Monitor and Evaluate:** This is crucial due to the non-deterministic nature of LLMs. It involves looking at traces, general observability, introspection into internals (especially for multi-turn or agentic systems), and evaluating at both individual call and business levels.\n\nAdditionally, a pattern for rolling out LLM applications is mentioned:\n\n1.  **Proof of concept (POC)**\n2.  **Risk assessment & cost evaluation**\n3.  **Controlled testing** (deploying as an internal tool)\n4.  **Monitoring setup & evaluation systems**\n5.  **Gather feedback**\n6.  **Limited deployment** (e.g., to 0.1% or 1% of users)\n7.  **Look more at performance metrics & get user feedback**\n8.  **Gradual expansion to full production**",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n"
    ],
    "duration_ms": 8412.701845169067,
    "timestamp": "2025-03-28T23:02:12.045082",
    "model_provider": "gemini",
    "context_tokens": 2478,
    "completion_tokens": 249,
    "id": "q6",
    "user_type": "ml_engineer",
    "scenario": "general",
    "judgment": "pass",
    "reason": "The response directly answers the question by outlining the stages of building an LLM application as discussed in the workshop. It provides specific details about the lifecycle stages, including specs, build, deploy, and monitor and evaluate. Additionally, it mentions the pattern for rolling out LLM applications, which includes proof of concept, risk assessment, controlled testing, and gradual expansion. The response is factually correct based on the workshop content and avoids any hallucinations or fabricated information.",
    "evaluation_type": "llm"
  },
  {
    "question": "What did Hugo say about evaluation and iteration in LLM applications?",
    "response": [
      "Based on the transcript sections provided:\n\nHugo described evaluation and iteration as key parts of the lifecycle for generative AI and LLM-powered apps, particularly after the deployment stage due to their non-deterministic nature.\n\n**Regarding Evaluation:**\n\n*   It follows the \"deploy quickly\" stage because you don't know exactly how the LLM will behave.\n*   It involves looking at data and results to quantify performance.\n*   It requires monitoring, logging, tracing, and general observability into the app and conversations.\n*   Evaluation should happen at multiple levels:\n    *   **Individual level:** Checking if the LLM call is correct (e.g., extracting the right information from a profile).\n    *   **Output quality level:** Assessing if the generated output is good (e.g., does an automated email look like one a human expert would write?). This often requires domain expertise (like getting an HR person to evaluate recruitment emails).\n    *   **Business level:** Determining if the application is serving the overall business goal (e.g., is the recruiter app actually helping recruit more efficiently and get better candidates?). Aligning outputs with business metrics is crucial.\n*   It's listed as a \"1st principle\" for the course.\n*   It's essential for tackling challenges like non-determinism and hallucinations, often involving leveraging domain expertise.\n*   Setting up evaluation systems is part of the controlled rollout process for LLM applications.\n\n**Regarding Iteration:**\n\n*   It follows the monitoring and evaluation phases.\n*   It involves making changes quickly based on evaluation findings.\n*   Methods include prompt engineering, fine-tuning, and changing business logic.\n*   It's listed as a \"1st principle\" for the course (\"Iterate quickly\").\n*   It's part of the strategy for handling non-determinism (iterating on prompts and context).\n*   The software development lifecycle for LLMs should be built to allow for quick iteration.\n*   Iteration happens concurrently with the gradual rollout of an LLM application.\n*   Week 2 of the workshop is dedicated to iteration, evaluation, and observability.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 11] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:27:43.690]\n    hugo bowne-anderson: The next 1st principle is evaluation. You need to look at your data and results and quantify performance. And this is a combination of domain expertise and often binary classification. And then you need to iterate quickly. So using prompt engineering, fine tuning, changing business logic, all of these things. So these are what we're going to index on. Ensure, we'\n    \n    re going to use llama index and bur and all these other things at a variety\n    hugo bowne-anderson: of points. And all the Apis today we're going to use Openai and Gemini right and Gradiosa front end. But those things will come and go. Give, give or take right. But these principles will remain constant.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: the way we're going to get out of all these challenges. Okay? So for non-determinism, we're going to log inputs and outputs evaluate logs. Iterate on prompts and context and use Api knobs to reduce variance. And we're going to do all of that this week. Okay for hallucinations. Once again, we need to log inputs and outputs in dev and prod. We need to leverage domain expertise to evaluate the output in dev and prod. So.\n    \n    hugo bowne-anderson: as I said before, if we're generating emails, it's not an engineer who should sit down and see whether these emails are good emails. It's the person who used to write the emails right.\n    \n    Get them in the loop\n    hugo bowne-anderson: for evaluation, the same as above for iteration. We're building a software development lifecycle that allows you to do all of these things quickly. And once again, business value. We need to align outputs with business metrics and optimize our workflow to achieve measurable roi. Okay.\n    \n    hugo bowne-anderson: now, the final thing I wanted to say, this is actually a shameless self promotion, but it's it's shameless because it's incredibly relevant. So a friend of mine, I don't know if anyone's seen this Alex trick Van Lyncherton from Zen, Ml.\n\n"
    ],
    "duration_ms": 14348.545789718628,
    "timestamp": "2025-03-28T23:02:31.398565",
    "model_provider": "gemini",
    "context_tokens": 2429,
    "completion_tokens": 431,
    "id": "q7",
    "user_type": "data_scientist",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question with specific details about what Hugo said regarding evaluation and iteration in LLM applications. It accurately reflects the workshop content by discussing the importance of evaluation and iteration in the lifecycle of generative AI and LLM-powered apps, particularly after deployment due to their non-deterministic nature. The response outlines the different levels of evaluation and the iterative process, aligning with the first principles mentioned in the workshop. It avoids hallucinations and provides a comprehensive overview of the topic.",
    "evaluation_type": "llm"
  },
  {
    "question": "What is the difference between LLM fine-tuning and prompt engineering according to the workshop?",
    "response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 22\n}\n]",
      ""
    ],
    "duration_ms": 1361.0920906066895,
    "timestamp": "2025-03-28T23:02:37.761437",
    "model_provider": "gemini",
    "context_tokens": 2482,
    "completion_tokens": 0,
    "id": "q8",
    "user_type": "student",
    "scenario": "technical",
    "judgment": "fail",
    "reason": "The response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the question about the difference between LLM fine-tuning and prompt engineering according to the workshop. The response fails to provide any specific details or factual information from the workshop content.",
    "evaluation_type": "llm"
  },
  {
    "question": "How should I think about data when building LLM applications?",
    "response": [
      "Based on the transcript sections provided:\n\nYou should think about data in the context of LLM applications primarily through the lens of monitoring, evaluation, and understanding the application's behavior, especially given its non-deterministic nature.\n\nHere's a breakdown:\n\n1.  **Capture Data:** A core principle mentioned is \"logging, monitoring, and tracing,\" stating explicitly, \"You need to capture data.\" This includes inputs and outputs of LLM calls, traces, and conversations happening within the app.\n2.  **Monitor and Observe:** Because LLMs are non-deterministic and their behavior isn't always predictable, you need to monitor them after deployment. This involves looking at traces and having general observability into the app and its conversations. You need visibility into the internals to understand what's happening at each point.\n3.  **Identify Failure Modes:** By \"looking at your data\" gathered through monitoring, you can identify where failure modes are occurring.\n4.  **Evaluate:** Data is essential for evaluation at multiple levels:\n    *   **Individual Level:** Checking if an individual LLM call is correct (e.g., extracting the right information).\n    *   **Business Level:** Assessing if the application is achieving its intended business goals (e.g., recruiting more efficiently), not just generating plausible outputs.\n5.  **Gather Feedback:** During rollout phases (like limited deployment), you should gather qualitative user feedback and look at performance metrics, which are forms of data used for iteration.\n6.  **Input/Output:** At a fundamental level, interacting with LLMs involves API calls with input and output, which is a form of data interaction.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n3. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "duration_ms": 12116.174936294556,
    "timestamp": "2025-03-28T23:02:54.882810",
    "model_provider": "gemini",
    "context_tokens": 2409,
    "completion_tokens": 336,
    "id": "q9",
    "user_type": "data_scientist",
    "scenario": "general",
    "judgment": "pass",
    "reason": "The response directly answers the question with specific details about how to think about data when building LLM applications. It accurately reflects the workshop content by emphasizing the importance of monitoring, evaluation, and understanding the application's behavior, especially given the non-deterministic nature of LLMs. The response avoids hallucinations and provides a structured breakdown of key principles, such as capturing data, monitoring, identifying failure modes, evaluating at different levels, gathering feedback, and understanding input/output interactions.",
    "evaluation_type": "llm"
  },
  {
    "question": "What were the main topics covered in the first workshop?",
    "response": [
      "Based on the provided transcript sections, the main topics covered in the first workshop were:\n\n*   **Foundations of LLM software development:** Introducing GenAI and LLM development.\n*   **Overview of the software development lifecycle** in the context of LLMs.\n*   **Using key tools and frameworks:** Specifically mentioned were Lama Index, Gradio, sqlite, and the dataset tool by Simon Willison.\n*   **Setting up a foundational app:** Building an MVP application for querying PDFs and generating responses.\n*   **Iterating on prompts.**\n*   **Adding interactivity** using Gradio.\n*   **Starting logging and visualizing logs.**",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n3. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n4. [Distance: 1.0000] [Chunk 60] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:50:39.550]\n    h\n    hugo bowne-anderson: a work outside a week, you'll get significantly more. If you do. 4\u00a0h outside per week you'll probably get like way way more. So of course it's January. We've all got a bunch of stuff on, but definitely leverage myself, Stefan, and the community we're building as well.\n    \n    What I\n    hugo bowne-anderson: also want to say we didn't get round to this, and we'll do it next time you can deploy this app on Gradio. And I've actually got a video here of me doing.\n    Stefan Krawczyk CEO DAGWorks Inc.: Tomorrow.\n    \n    hugo bowne-anderson: Yes, sorry on Modal. Exactly. And you need to have we need to have given you credentials, and so we'll get those, and so we'll get that sorted.\n    \n    hugo bowne-anderson: Everything we need in the modal form.\n    \n    hugo bowne-anderson: And I'll do that next time.\n    \n    And then\n    hugo bowne-anderson: the final thing I would very much appreciate. If you all took 3\u00a0min now, I don't know why it has this big gap here. We'd love feedback on this session, so I'd like to know? How do you rate the pace? What did you like most? What could we improve anything unclear or confusing? If so, what this will help us create the best course going forward for all of you, and and both of us as well. To be honest. The other thing I'm not sure I mentioned. But this is the 1\n    \n    st time\n    hugo bowne-anderson:\n    \n    Stefan and I are teaching this long version of\n    hugo bowne-anderson: of this material. We actually taught a 3\u00a0h version of it recently in Austin, at the generative AI summit to 100 people, which was just so wonderful to do that in person, and Stefan and I have been teaching and working on these things for a long time, but this course, as it exists, it is the 1st cohort of it, so there may be rough edges. I'd be very surprised I'd be concerned if there weren'\n\n5. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n"
    ],
    "duration_ms": 8134.107828140259,
    "timestamp": "2025-03-28T23:03:08.018556",
    "model_provider": "gemini",
    "context_tokens": 2462,
    "completion_tokens": 136,
    "id": "q10",
    "user_type": "student",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question by listing specific topics covered in the first workshop. It includes details such as the introduction to GenAI and LLM development, the software development lifecycle, key tools and frameworks, setting up a foundational app, iterating on prompts, adding interactivity, and starting logging. The information is factually correct and aligns with the workshop content, avoiding any hallucinations or fabricated content.",
    "evaluation_type": "llm"
  },
  {
    "question": "What advice did Hugo give about integrating LLMs into existing workflows?",
    "response": [
      "Based on the provided transcript sections, Hugo discusses the lifecycle for building and deploying generative AI and LLM-powered apps, contrasting it with traditional software development. He mentions building involves prompt engineering, embeddings, fine-tuning, and ensuring business logic is satisfied. He also describes a gradual rollout pattern for LLM applications (POC -> internal testing -> limited deployment -> full production).\n\nHowever, the sections do not contain specific advice from Hugo on *how* to integrate LLMs into *existing* workflows.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n5. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n"
    ],
    "duration_ms": 10137.42208480835,
    "timestamp": "2025-03-28T23:03:23.161328",
    "model_provider": "gemini",
    "context_tokens": 2479,
    "completion_tokens": 101,
    "id": "q11",
    "user_type": "ml_engineer",
    "scenario": "factual",
    "judgment": "fail",
    "reason": "The response does not directly answer the question about Hugo's specific advice on integrating LLMs into existing workflows. While it mentions aspects of building and deploying LLM applications, it lacks specific details on integration into existing workflows, which is the core of the question. Additionally, it acknowledges that the sections do not contain specific advice from Hugo on this topic, indicating a failure to address the key part of the question.",
    "evaluation_type": "llm"
  },
  {
    "question": "How does observability play a role in LLM application development?",
    "response": [
      "Based on the provided transcript sections:\n\nObservability plays a role after the initial deployment of LLM-powered apps. Due to the non-deterministic nature of these applications (meaning their behavior isn't always predictable), monitoring and evaluation are crucial.\n\nObservability involves:\n\n1.  **Looking at traces:** To understand the flow and interactions within the app.\n2.  **General observability:** Gaining insight into the app and the conversations taking place.\n3.  **Visibility into internals:** Being able to introspect what's happening at each point, especially in complex setups like multi-turn conversations or agentic systems.\n4.  **Identifying failure modes:** By examining the data captured through monitoring, developers can see where the application is failing.\n5.  **Capturing data:** Logging, monitoring, and tracing are highlighted as a core principle, emphasizing the need to capture data.\n\nSetting up monitoring and evaluation systems is also mentioned as a specific step in the typical rollout process for LLM applications in industry.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n4. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "duration_ms": 9021.804094314575,
    "timestamp": "2025-03-28T23:03:37.187586",
    "model_provider": "gemini",
    "context_tokens": 2428,
    "completion_tokens": 202,
    "id": "q12",
    "user_type": "data_scientist",
    "scenario": "technical",
    "judgment": "pass",
    "reason": "The response directly answers the question by explaining the role of observability in LLM application development. It provides specific details about the importance of monitoring and evaluation due to the non-deterministic nature of LLMs. The response outlines key aspects of observability, such as looking at traces, gaining general insight, visibility into internals, identifying failure modes, and capturing data. These points align with the workshop content and emphasize the necessity of setting up monitoring and evaluation systems, making the response factually correct and relevant.",
    "evaluation_type": "llm"
  },
  {
    "question": "What are the key considerations for production deployment of LLM applications?",
    "response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 15\n}\n]",
      ""
    ],
    "duration_ms": 2014.2388343811035,
    "timestamp": "2025-03-28T23:03:44.203886",
    "model_provider": "gemini",
    "context_tokens": 2428,
    "completion_tokens": 0,
    "id": "q13",
    "user_type": "ml_engineer",
    "scenario": "general",
    "judgment": "fail",
    "reason": "The response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the key considerations for production deployment of LLM applications. The response fails to provide any specific details or factual information based on the workshop content, making it unacceptable.",
    "evaluation_type": "llm"
  },
  {
    "question": "How should I handle errors and edge cases in LLM applications?",
    "response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 9\n}\n]",
      ""
    ],
    "duration_ms": 1854.4797897338867,
    "timestamp": "2025-03-28T23:03:51.063577",
    "model_provider": "gemini",
    "context_tokens": 2461,
    "completion_tokens": 0,
    "id": "q14",
    "user_type": "student",
    "scenario": "technical",
    "judgment": "fail",
    "reason": "The response provided is an error message related to exceeding API quotas, which does not address the question about handling errors and edge cases in LLM applications. It is off-topic and does not provide any specific details or insights from the workshop content on how to handle such situations. The response fails to meet the evaluation criteria as it does not directly answer the question or provide relevant information.",
    "evaluation_type": "llm"
  },
  {
    "question": "What did the workshop say about the importance of looking at your data?",
    "response": [
      "Based on the workshop transcript sections provided:\n\nThe transcript emphasizes the importance of looking at your data, particularly in the context of generative AI and LLM-powered apps, due to their non-deterministic nature.\n\n*   **Monitoring and Evaluation:** It's stated that after deploying these apps, you must monitor and evaluate them because \"We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next\" [00:23:32.990].\n*   **Observability and Introspection:** Monitoring involves looking at traces and having general observability. You also need visibility into the internals, especially for complex setups like multi-turn conversations or agentic systems, to \"introspect into what's happening at each point\" [00:23:32.990].\n*   **Identifying Failure Modes:** The transcript explicitly states, \"once you start looking at your data there, you'll see where the failure modes are right\" [00:23:32.990]. It later reiterates this point regarding multi-turn conversations or multi-agents: \"If you put things in spreadsheets and look at them, you will see the failure modes\" [01:29:11.080].\n*   **Manual Annotation and Workflow:** A workflow is described where LLM responses are put into spreadsheets, annotated daily by a domain expert as correct or incorrect, and errors are highlighted. This manual inspection is deemed \"incredibly important before getting to the step of automating\" [01:29:11.080].\n*   **Validating Generated Data/Prompts:** Even when using LLMs or combinatorial methods to generate data or prompts synthetically, the transcript advises getting \"yourself in the loop\" or the \"domain expert in the loop\" to review and ensure the generated items are sensible and appropriate, comparing unintelligent use to following Google Maps down a one-way street [01:23:59.200].",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 47] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:29:11.080]\n    hugo bowne-anderson: Wrong?\n    \n    hugo bowne-anderson: Okay, you get the picture. And then maybe there was one which is.\n    \n    hugo bowne-anderson: yeah,\n    \n    hugo bowne-anderson: different to response,\n    \n    one\n    hugo bowne-anderson: which was the same question. So you can see now. And one of the reasons I want to work in a sheet and not in pandas is the ability to annotate this stuff. And, in fact.\n    \n    hugo bowne-anderson: for most of us a golden rule of working with spreadsheets is never to highlight rows. But if there's something that's really wrong,\n    \n    you should probably like\n    hugo bowne-anderson: this isn't even extracted as a list. Right? So Whoa.\n    \n    hugo bowne-anderson: yeah, I want to like, make that red. So I can share that with friends or colleagues and be like, Hey, we need to work on this. Okay?\n    \n    hugo bowne-anderson: Similarly, I will tell. It was Philip who I mentioned earlier from Honeycomb.\n    \n    I actually hung out with him at Kubecon EU a lot\n    hugo bowne-anderson: last year, and I was going to say earlier this year, but it was last year in Paris,\n    \n    and he told me that what what Hamill actually got him to do was\n    hugo bowne-anderson: almost daily work in spreadsheets where they got Llm. Responses, he would annotate them, say, what's correct and wrong, then put it through another Llm. With his responses, then turn that into an Llm. As judge, and he would daily, as the domain expert, get in spreadsheets and say, correct, incorrect, correct, incorrect, and they developed a system where they would flow like like that. Right?\n    \n    I think these types of workflows are incredibly important before getting to the step of automating right and\n    hugo bowne-anderson: to my point earlier. If someone, if any of you was like, how do I figure out how to evaluate my multiturn conversations or my multi agents. If you put things in spreadsheets and look at them, you will see the failure modes like you. If if you say, extract it as a list, and it never extracts it as a list.\n\n2. [Distance: 1.0000] [Chunk 54] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:40:17.490]\n    hugo bowne-anderson: so Gp has a good question checking the understanding.\n    \n    If you develop using code spaces and deploy using gradio that link is publicly available on the\n    hugo bowne-anderson: Internet,\n    Stefan Krawczyk CEO DAGWorks Inc.: You want. I mean, we can check that. Does anyone wanna.\n    \n    hugo bowne-anderson: Yeah. Why don't I share mine? Yeah, I.\n    Stefan Krawczyk CEO DAGWorks Inc.: Oh, yeah.\n    \n    hugo bowne-anderson: I think it is. Yeah, so I wouldn't share it with anyone.\n    Stefan Krawczyk CEO DAGWorks Inc.: If you click that you should see. Yeah, you should be able to see my app. But this is where in codespace. Obviously, you can.\n    Stefan Krawczyk CEO DAGWorks Inc.: cancel, or at least rather kill the process. And then, yeah.\n    \n    hugo bowne-anderson: Exactly.\n    Stefan Krawczyk CEO DAGWorks Inc.: 4, 4. Okay, no. So it knows. Local to me. Okay.\n    \n    hugo bowne-anderson: Oh, interesting!\n    Stefan Krawczyk CEO DAGWorks Inc.: Okay. Okay.\n    \n    hugo bowne-anderson: So we do need to move on. I did want to say the group I I chilled with so many fascinating things. 2\n    \n    takeaways were\n    hugo bowne-anderson: increase specificity. So if you ask actually, Yuan, would you mind giving your example.\n    \n    Yuan Liu: Sure, it's just a very high level example. For example, I just use Stephen's resume in the Pdf format and upload it. If I just ask, say what the Pdf. Is about. It just will give me a random answer that is not relevant, however, though if I add some prefix, say you are an AI assistant, and you are only allowed to answer, based on the provided information, and then attach your question. After that the answer is actually what we are looking for.\n    \n    hugo bowne-anderson: Exactly so increase special.\n    \n    And this is\n    hugo bowne-anderson: this is a reasonable assumption.\n\n3. [Distance: 1.0000] [Chunk 30] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:02:22.870]\n    hugo bowne-anderson: also? What type of salary you want to command as well. We live in a pretty strange world, where you know certain niche skills that you can learn pick up relatively easily, which are gate kept in a number of ways, can, you know, really increase your salary? And everyone we need to think about think about those things.\n    \n    hugo bowne-anderson: I will also, actually, no, I'll leave it.\n    \n    hugo bowne-anderson: Leave it at that, because I've got so many things coming to my mind, and I do want to continue. I haven't looked at this yet I just opened this.\n    \n    Can people see\n    hugo bowne-anderson: a bunch of? I'm so sorry for showing pie charts in a data centric course as well. That seems like a big no, no. But hey, Google, we have some people from Google here. Can you? Can you issue a Pr into Google forms to not make pie charts, please.\n    \n    hugo bowne-anderson: But anyway, so this is really exciting. We've got some really nice slices of the pie here. Okay, so we've got 40% of people haven't built or deployed, l empowered apps. Yet 22%\n    \n    have\n    hugo bowne-anderson: experimented locally but not deployed, and 40% have built and deployed applications. This is fantastic. Now, what this means, though, is that tailoring, as we'll see tailoring the content to everyone.\n    \n    hugo bowne-anderson: won't be straightforward, due to the variance here, but we'll definitely do our best, and if we're going too slow or too fast, we've got little surveys. At the end of each workshop. We'll figure out how to align with everyone on your needs as students as well. Okay, so have you worked with gradio or streamlit or similar tools to build front end interfaces for AI or data apps. No, I haven't.\n    \n    hugo bowne-anderson: 35% tried them. 40, 20% of have built apps very cool.\n\n4. [Distance: 1.0000] [Chunk 43] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:23:59.200]\n    hugo bowne-anderson: Then I'm going to write the results to a Csv.\n    \n    hugo bowne-anderson: and then what I'm going to do is when I write the query, I'm going to generate 20 queries to loop. So please generate 20, but not a thousand. If you're using the Api key, I provided. What you'll see is that I'm taking these base queries and then adding stuff like, Please return it in. Json. Extract it as a list focus on recent jobs. So what you'll note is, I'm actually synthetically generating prompts in some way. Now, I'm not doing it quite using an Llm. Right? I'm doing it combinatorially with these. Having said that.\n    \n    hugo bowne-anderson: I generated this exercise by chatting about it with Claude and Chatgpt, and so I chatted with Llms. For a while, and then decided this and and got Chat gpt to generate the base queries, for example. So don't don't be scared of using Llms in your workflow, but use them smartly.\n    \n    hugo bowne-anderson: What I'm going to tell you is a very silly example. But years ago I was in a car with a family member driving,\n    \n    and\n    hugo bowne-anderson: they nearly drove down a 1-way street.\n    \n    hugo bowne-anderson: and I said to them, you nearly drove down a 1-way street, and they said, Google Maps told me to drive there.\n    \n    hugo bowne-anderson: And what I'm trying to illustrate is that we can use. That's that's an example of an unintelligent use of technology. And sure there are a lot of other things at play, being in like rush hour traffic all of these things. But if you start using technology intelligently, like, if this had suggested a base query.\n    \n    Tell me about\n    hugo bowne-anderson: their childhood. I would be like, Oh, that's silly, because we're looking at Linkedin profiles. Right? So getting synthetically generated. Data isn't necessarily bad, but you need to get yourself in the loop to make sure. That is what you would want it to do. Get the domain expert in the loop. Right? So all that having been said.\n\n5. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n"
    ],
    "duration_ms": 12198.583126068115,
    "timestamp": "2025-03-28T23:04:08.272227",
    "model_provider": "gemini",
    "context_tokens": 2494,
    "completion_tokens": 402,
    "id": "q15",
    "user_type": "data_scientist",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question by highlighting the importance of looking at your data in the context of generative AI and LLM-powered apps. It provides specific details from the workshop transcript, such as the need for monitoring and evaluation due to the non-deterministic nature of these systems, the importance of observability and introspection, identifying failure modes, and the manual annotation workflow. The response is factually correct and avoids any hallucinations or fabricated content, making it an acceptable answer.",
    "evaluation_type": "llm"
  },
  {
    "question": "How can I measure the success of my LLM application?",
    "response": [
      "Based on the transcript sections provided:\n\nYou can measure the success of your LLM application on multiple levels:\n\n1.  **Monitoring and Observability:** Set up monitoring to look at traces, have general observability into your app and conversations, and visibility into the internals, especially for complex systems. This helps identify failure modes.\n2.  **Individual Call Evaluation:** Evaluate at an individual level to ensure the LLM call itself is correct (e.g., extracting the correct information from a source).\n3.  **Output Quality Evaluation:** Assess the quality of the generated output. This might involve domain experts (e.g., having an HR person review emails generated by a recruiter app).\n4.  **Business Level Evaluation:** Ultimately, measure success by determining if the application is serving the business goal (e.g., for a recruiter app, is it helping recruit more efficiently and get better candidates?).\n5.  **User Feedback:** Gather qualitative user feedback during deployment phases.\n6.  **Performance Metrics:** Look at performance metrics during rollout.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n2. [Distance: 1.0000] [Chunk 12] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:29:34.600]\n    Recently they published a database\n    hugo bowne-anderson: of it was over 300 Llm. Deployments in industry. It's totally wild. It's incredible. And now they've just updated it so that it's 420 of them. Sorry my screen.\n    \n    I do\n    hugo bowne-anderson: want to.\n    \n    hugo bowne-anderson: It is fascinating because you can, right.\n    \n    hugo bowne-anderson: And they've linked to all these case studies. So you can just go into Alaska airlines, Nlp flight search implementation and see what's happening there. I very much encourage everyone to go through this.\n    \n    But\n    hugo bowne-anderson: what's relevant for us is that? Well, I'm doing a live stream with him later today, which I've linked to in resources all about this on discord, the resources channel. So that's the shameless self promotion. But that's really, by the by, what one thing and this wasn't even in my slide deck originally. But I was reading this this morning, and I was just like, Okay, look at the way people. This is a pattern they've identified for how people roll out Llm applications. Right? So you have a proof of concept. And then you do a risk assessment cost evaluation.\n    \n    hugo bowne-anderson: Then you do controlled testing and deploy it as an internal tool to some people internally. Right?\n    \n    hugo bowne-anderson: Then you really do your monitoring setup and evaluation systems. Then you gather feedback. Then you do a limited deployment. You roll it out to, you know, one or 0 point 1% of users something like that. And then you look more at performance metrics get user feedback qualitative user feedback as well. And then you do a gradual expansion to full production.\n    \n    So I just want to make clear that\n    hugo bowne-anderson: there's a huge continuum, if not multi dimensional space of what it means to deploy an Llm. Powered applications. It's not. It's not on or off right? And so we have this incredibly gradual rollout, right? But at the same time we're iterating on on the product. So that's at least 2 dimensions of what we mean by by production. And these are the types of things we'll see here in this course as well.\n\n3. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n4. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n5. [Distance: 1.0000] [Chunk 4] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:09:56.800]\n    t\n    hugo bowne-anderson: been added yet. If you cannot access this, please message me your github handle on zoom, and this is the way we're going to do things. All course, all correspondence during sessions will be on zoom otherwise on discord. Okay.\n    \n    hugo bowne-anderson: so please do. Message me your github handle and we'\n    \n    ll get started on that in\n    hugo bowne-anderson: I'll add all of those once we have a look at this 1st notebook.\n    \n    hugo bowne-anderson:\n    \n    So\n    hugo bowne-anderson: I wanted to start just by saying once again welcome to the course. Stefan and I are deeply excited about building Llm applications, leveraging modern technology to build robust software and helping other people build Llm applications. I've been educating data scientists and machine learning engineers and Ml people.\n    \n    Machine learners quote unquote\n    hugo bowne-anderson: for over a decade now, and I haven't had a strong opening to be able to educate software engineers to to be honest about the data stuff. And I think with Llms, it's an incredible opportunity for a lot of people from the software side and product side in all honesty to come in and start learning a lot more about data, science and machine learning. And AI. So that's why we frame this as being very valuable for software engineers as well.\n    \n    hugo bowne-anderson: I do want to say what this course is about and what it isn't. I think we all have a sense of what it's about. But I just want to make clear to manage expectations. So we're going to be providing a practical 1st principles. Approach to building Llm. Powered apps. So focusing on all the 1st principles which I'll get to in a second, but workflows iteration and hands-on development. This is a space to talk about and to build production grade systems that scale beyond Pocs, proof of concepts and purgatories.\n    \n    hugo bowne-anderson: And it's also an opportunity not only to learn from practitioners such as myself and Stefan, but you may have seen,\n\n"
    ],
    "duration_ms": 10553.632020950317,
    "timestamp": "2025-03-28T23:04:23.831205",
    "model_provider": "gemini",
    "context_tokens": 2428,
    "completion_tokens": 208,
    "id": "q16",
    "user_type": "ml_engineer",
    "scenario": "general",
    "judgment": "pass",
    "reason": "The response directly answers the question with specific details on how to measure the success of an LLM application. It covers multiple levels of evaluation, including monitoring and observability, individual call evaluation, output quality evaluation, business level evaluation, user feedback, and performance metrics. These points align with the principles discussed in the workshop, making the response factually correct and relevant. Additionally, the response avoids any hallucinations or fabricated content.",
    "evaluation_type": "llm"
  },
  {
    "question": "What was discussed about API parameters and their effects on LLM outputs?",
    "response": [
      "Based on the provided transcript sections, there is no discussion about specific API parameters or their effects on LLM outputs. The sections mention making API calls, using vendor APIs, and the non-deterministic nature of LLMs (same input can lead to different outputs), but they do not go into detail about parameters like temperature, top_p, etc., or how they influence the results.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 10] \n    [TIMESTAMP: 00:25:14.560]\n    hugo bowne-anderson: Then the next level is,\n    \n    does it generate an email that\n    hugo bowne-anderson: we like? And you probably want to get the domain expert who used to write the emails in on that someone from Hr or the people team, as we call them these days. Right? Because Hr human resources was one of the worst names possible. We changed it to make them seem they're on our side.\n    \n    hugo bowne-anderson: But all jokes aside at the next level, though, just because it looks like an email that could work.\n    \n    hugo bowne-anderson: That isn't the business goal. The business goal is to be able to recruit more efficiently and get better candidates. So in the end, we're not just interested in evaluating individual Llm calls. We want to make sure that it's serving the business right. And these are the types of ways we're going to be approaching, thinking through evaluations in this course as well. So thinking about what the 1st principles we're going to be indexing on in this course are.\n    \n    hugo bowne-anderson: we have Api calls. We're pinging Llms, we have input and output. Okay, Llms, that's the 1st principle. Second principle is, they're non-deterministic. You can have same inputs and different outputs.\n    \n    This is not software\n    hugo bowne-anderson: as far as or it's not what we think of as software. And weirdly. Llms are not often what we think of as computers, right like we've seen that they can be particularly bad at arithmetic, and like counting the number of R's in the word strawberry and stuff like that like that is mind boggling. We really need to reconfigure our relationship with these technologies because we expect computers to be good calculators. But Llms are not always right. The next principle is logging, monitoring, and tracing. You need to capture data.\n\n2. [Distance: 1.0000] [Chunk 9] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:23:32.990]\n    But I think\n    hugo bowne-anderson: you know all models, all reductions, are wrong, some are useful. I think this is a particularly useful one, especially when comparing it with the Llm lifecycle. You have specs you build, you test. You deploy iterate occasionally, depending on changing specs.\n    \n    That type of thing\n    hugo bowne-anderson: for generative AI and Llm powered apps. We have specs, and then we build. And then we deploy quickly. Then we have to monitor and evaluate because of the non-deterministic nature of these these things. We actually don't know how it's going to behave, and if it works well one day it may have failure modes the next, and we'll see examples of that today, actually. And what do all these things involve?\n    \n    So building can involve\n    hugo bowne-anderson: prompt engineering embeddings, fine tuning, making sure your business logic is satisfied, deploying. You want to think about unit tests and continuous integration and versioning. When you start to monitor,\n    \n    you want to look at traces and have general observability into\n    hugo bowne-anderson: your app and all the conversations that take place in there. If we're talking about Llms,\n    \n    and\n    hugo bowne-anderson:\n    \n    you also want to have\n    hugo bowne-anderson: visibility in the into the internals. So you know, people come and ask about multi-turn conversations, and like 10\n    \n    hugo bowne-anderson: Llms connected to form something agentic or something along those lines. Right? And what you want in those cases is to be able to introspect into what's happening at each point. And once you start looking at your data there, you'll see where the failure modes are right. And then, when you think about evaluation, you want to evaluate at an individual level,\n    \n    making sure that\n    hugo bowne-anderson: the Llm. Call is is correct, you also want to be able to evaluate at a business level. So one example that will come to time and time again is a recruiter app where it consumes Linkedin profiles and automates, emails to people for potential roles. And so think about this at an individual call level? You could, the question could be, does it get out the correct information from the Linkedin profile.\n\n3. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n4. [Distance: 1.0000] [Chunk 23] Speaker: Nathan. \n    [TIMESTAMP: 00:51:30.100]\n    m going to say is that\n    hugo bowne-anderson: once again, this is something that's not binary, right? It isn't agentic or not. There's some sort of continuum to agents. And, Stefan, maybe you can tell us about the augmented Llm. As kind of a stepping stone to agents, and then we can move on.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, yeah, I mean, so augmented Llms are, really, if if you had the. So an agent is trying to make a decision, but you can get an Lm to, you know, extract stuff, and maybe even what's called tool calling right? And so that's not really agentic. You're just, basically, you know, using the Lm. To help you with some sort of workflow. So, for example, a good one is. If you have a receiving Pdfs\n    Stefan Krawczyk CEO DAGWorks Inc.: and say it's a accounts payable. And so you want to extract things from it right? You can use an Lm plus maybe some other tooling or other things to kind of extract data from it. That is not\n    Stefan Krawczyk CEO DAGWorks Inc.: an agent. It is, you know, augmented Lm, and potentially adding a few things to you know. Get your kind of workflow done. The other thing to kind of mention on the agents front is like in in with respect to this course, like an agent underneath is actually has some Lm call within it plus some business logic. And so this is where you could kind of think of on that continuum as you slowly\n    Stefan Krawczyk CEO DAGWorks Inc.: make things more complex with Lms, you eventually get to the agent point. But you're not gonna have something reliable. If you can't get the individual components that the agent is comprised of to work well. And so this is where you know part of the the 1st principles thinking and and what we're kind of bringing or helping to teach you in this course is that, if you can get the fundamentals right, you'll be then be able to progress from simple workflows like this to more more actual agent ones.\n    \n    hugo bowne-anderson: Exactly and the other thing, I'll just add,\n\n5. [Distance: 1.0000] [Chunk 36] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:11:47.200]\n    hugo bowne-anderson: you'll see this. Okay, so we'\n    \n    re going to navigate to workshop one\n    hugo bowne-anderson: into the Apps directory and what you'll see is that there's a file here in workshops, in.\n    \n    hugo bowne-anderson: in workshops. One apps called Dot, N. So that's where we want to put our Api keys. I've got an Openai Api key here, and I've got a Google Gemini key as well. Stefan, would you mind getting a thumbs up and thumbs down from people of whether they have their own open. AI Api key.\n    Stefan Krawczyk CEO DAGWorks Inc.: Yeah, go to your chat on your own open AI,\n    Stefan Krawczyk CEO DAGWorks Inc.: so go to the chat and you react thumbs up thumbs down to my question, or give us a sense whether you have it.\n    \n    hugo bowne-anderson: And for those that do not have Openai Api keys.\n    \n    Shame on Openai for\n    hugo bowne-anderson: not having a free tier to be honest, because wouldn't that be wonderful if they let everyone just play around with it briefly,\n    \n    but I will give you\n    hugo bowne-anderson: okay. I'm gonna put like 10 bucks on it or something.\n    \n    So\n    hugo bowne-anderson: just know this is a shared resource. And if you want to use it more, please get your own Openai Api key. You can also use Google Gemini, which we will in a second.\n    \n    hugo bowne-anderson: and it has a free tier as well. But this one I'll share in a second. What am I naming it?\n    \n    hugo bowne-anderson: lm, sdlc.\n    \n    hugo bowne-anderson: so all I'm saying is, please don't tax this one a huge amount, so I'm going to show an example where we can do a for loop to ping the Api a bunch of times in order to see non-determinism at play. Please do not do that for loop a thousand times, for example.\n    \n    hugo bowne-anderson: So I'm putting the Api key. They're getting longer and longer, aren't they?\n\n"
    ],
    "duration_ms": 9477.596282958984,
    "timestamp": "2025-03-28T23:04:38.313670",
    "model_provider": "gemini",
    "context_tokens": 2433,
    "completion_tokens": 78,
    "id": "q17",
    "user_type": "student",
    "scenario": "technical",
    "judgment": "pass",
    "reason": "The response directly answers the question by stating that there was no discussion about specific API parameters or their effects on LLM outputs in the provided transcript sections. It accurately mentions the relevant points from the transcript, such as making API calls, using vendor APIs, and the non-deterministic nature of LLMs, without fabricating any details about specific parameters like temperature or top_p. The response is factually correct and avoids hallucinations.",
    "evaluation_type": "llm"
  },
  {
    "question": "What ethical considerations should I keep in mind when building LLM applications?",
    "response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 14\n}\n]",
      ""
    ],
    "duration_ms": 1815.9003257751465,
    "timestamp": "2025-03-28T23:04:45.132474",
    "model_provider": "gemini",
    "context_tokens": 2409,
    "completion_tokens": 0,
    "id": "q18",
    "user_type": "data_scientist",
    "scenario": "general",
    "judgment": "fail",
    "reason": "The response does not address the question at all. Instead, it provides an error message related to API rate limits, which is off-topic and does not contain any information about ethical considerations for building LLM applications. It fails to meet the evaluation criteria of directly answering the question with specific details and is therefore unacceptable.",
    "evaluation_type": "llm"
  },
  {
    "question": "How did the first workshop address the challenge of hallucinations in LLMs?",
    "response": [
      "Sorry, an error occurred with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 9\n}\n]",
      ""
    ],
    "duration_ms": 799.5350360870361,
    "timestamp": "2025-03-28T23:04:50.937298",
    "model_provider": "gemini",
    "context_tokens": 2419,
    "completion_tokens": 0,
    "id": "q19",
    "user_type": "ml_engineer",
    "scenario": "technical",
    "judgment": "fail",
    "reason": "The response does not address the question at all. Instead, it provides an error message related to API rate limits, which is irrelevant to the content of the workshop or the challenge of hallucinations in LLMs. It fails to provide any specific details or factual information from the workshop content, making it unacceptable.",
    "evaluation_type": "llm"
  },
  {
    "question": "What examples or case studies were presented in the first workshop?",
    "response": [
      "Based on the provided transcript sections, the main example built during the first workshop session was a foundational app for **querying PDFs and generating responses**.\n\nThe transcript mentions that in the first week, they would:\n*   Set up this foundational app ([TIMESTAMP: 00:43:32.890]).\n*   Build the MVP (Minimum Viable Product) of this app ([TIMESTAMP: 01:48:38.250]).\n*   Iterate on prompts for this app ([TIMESTAMP: 01:48:38.250]).\n*   Add interactivity using Gradio ([TIMESTAMP: 01:48:38.250]).\n*   Start logging and visualizing logs related to this app ([TIMESTAMP: 01:48:38.250]).\n\nNo other specific examples or case studies were described as being presented in the first session within these transcript sections.",
      "Sources (most similar first, lower distance = more similar):\n1. [Distance: 1.0000] [Chunk 18] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:43:32.890]\n    hugo bowne-anderson: Also. Additionally, this kind of mirrors, real world development environments where teams work across multiple platforms. And I'm sure we've all been. I don't know how many of you have like worked in. I've worked in several startups that go like from 4 people to 150, and seeing the amount of tools introduced in that type of growth is mind boggling. I actually think that may be why startups fail because they don't adopt tools mindfully, but that's for another day.\n    \n    hugo bowne-anderson: I'll say a bit about the high level structure, and then I'll get you to complete a survey, and then we'll jump into executing some code together. So this is week one. Clearly, it's foundations of Llm software development. So we'\n    \n    re going to introduce you to Genai and Llm development\n    hugo bowne-anderson: give an overview of the software development lifecycle. Use some key tools and frameworks for Llm.\n    \n    Based applications and set up the foundational app which is querying Pdfs and generating responses\n    hugo bowne-anderson: today will be introductory and building this app. Then, on Wednesday or Thursday, depending what time zone you're in, I'm in Australia. So it's actually Tuesday morning here, soon to be Tuesday afternoon. But I was in America recently, which is why I'm saying Tuesday and not Tuesday.\n    \n    hugo bowne-anderson: We have several guest lectures, as you're aware.\n    \n    hugo bowne-anderson: the final part of our second session this week will be Charles Fry, from modal talking about hardware for Llm. Developers. And I recently did a podcast with him. And he's he'\n    \n    s just\n    hugo bowne-anderson: super wonderful and bright on this. Next week we'll be doing iteration, evaluation and observability. And the 1st lesson will be evaluation and feedback loops the second one on observability and debugging next week next. So we'll be doing a lot of prompt engineering in the second session this week in the 1st session next week, Sander Schulhoff, from learnprompting.org,\n    \n    and the prompt report will give a guest lecture on prompt engineering in the Llm software development lifecycle\n    hugo bowne-anderson: Week 3, we'\n\n2. [Distance: 1.0000] [Chunk 31] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:03:25.470]\n    hugo bowne-anderson: 10% aren't familiar. 57% understand the basics. I really, I wanted to ask that question before showing the slides. I wonder whether this slice of the pie is people like, Oh, yeah, I understand it now, because you just said something about it. But 30% regularly applying an Sdlc at work.\n    \n    hugo bowne-anderson: we can go through the rest of these. But it's the same story. We've got the same kind of amount of variance across all these dimensions, which is a significant variance, but it's the same across all the dimensions. So I think that that makes for a really really nice course from my experience. So we want people want to learn how to implement Llm apps, basic assistant type agents that can help a user achieve a set of predefined tasks, logging tracing inputs, outputs,\n    \n    setting up evals for Llm apps great\n    hugo bowne-anderson: evals, evaluation, building, monitoring and evals, agents, observability.\n    \n    hugo bowne-anderson: end-to-end development test driven Gen. AI development. So we'll be doing all these things. The one other thing I'll add to Tdd test. Driven development is Edd evaluation, driven development which we're going to be doing a bunch of as well. But what I'm seeing here is we want to know about logging and evaluation and looking at your data and observability and productionizing, which is all the things we're we're here for,\n    \n    which is\n    hugo bowne-anderson: super exciting.\n    \n    So\n    hugo bowne-anderson: we've been here for an hour, and I'm really itching to start to execute some code and playing with some apps. I've got way. Too many tabs open now. So I'\n    \n    m going to\n    hugo bowne-anderson: close them judiciously now, actually, Stefan, because I've already started a code. Space.\n    \n    Would you mind\n    hugo bowne-anderson: sharing your screen, and we can walk 3 people through how to spin up a codespace.\n    \n    hugo bowne-anderson: and then I'll go back to screen sharing, and I'll.\n    Stefan Krawczyk CEO DAGWorks Inc.: And.\n\n3. [Distance: 1.0000] [Chunk 59] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 01:48:38.250]\n    to your Point\n    hugo bowne-anderson: way, more lightweight than than Django admin. And funnily, I mean Simon Willison is one of the only people python for me is 2 things I know it's more to. But you've got the framework side, and you have the data side, right? And all the Ml. By data, I put data Ml, AI and Simon Willison co-created Django, and is like a force of nature in the data side as well. He's 1 of the only people who's really, you know, on both sides of of this thing.\n    \n    hugo bowne-anderson: so we are at time. I do want to just wrap up by firstly saying, Thank you for such a fun. Wonderful 1st session, all the great questions getting up and running. I had super fun in the breakout room and look forward to more with the rest of you in the next session. But just what we've done is we've built the Mvp. We've iterated on prompts. We've added interactivity with gradio started logging started visualizing logs.\n    \n    And to once again\n    hugo bowne-anderson: want to make clear. This is not about frameworks. It's about the types of principles we're talking about through looking at data and evaluation. As you see, we're using frameworks all the time, though, and it's so great that a bunch of you had never used gradio or llama index these types of things before. And you're already able to be up and running with these things. Okay, there are all types of optional homeworks you can do. I'm sorry. I also should have mentioned at the start.\n    \n    hugo bowne-anderson: There is no need to do any homework if you were to come here 2\u00a0h twice a week or watch the videos afterwards.\n    \n    hugo bowne-anderson: You would get a huge amount out of this. I hope right?\n    \n    hugo bowne-anderson: Of course.\n    \n    hugo bowne-anderson: the more you do outside, the more you get out of it. So the way I think about it is, if you're also able to do 2\n\n4. [Distance: 1.0000] [Chunk 6] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:15:16.300]\n    and people\n    hugo bowne-anderson: communicate and learn from each other, and I've already seen hints of this on discord before our 1st session. I've seen people chatting who are both in DC. And may meet up. I've corresponded with a couple of people in Sydney about meeting up. I've been seeing people ask each other about each other's work. Just go to the introductions Channel. If you haven't, and see what everyone's working on, and the variance of what everyone's working on. And it's incredibly exciting to me.\n    \n    hugo bowne-anderson: So I think that's something we can get out of this course as well.\n    \n    So\n    hugo bowne-anderson: what this course isn't. It is not a deep dive into every Llm. Tool or trend. Of course we'll be talking about rag and agents and embeddings. But the focus is on workflows, not an exhaustive tool coverage. And actually, on top of that, in today's session, we use Lama Index gradio sqlite, a tool by Salem and Willison, called dataset. We may even get to modal for deployment. We'll use one or 2 vendor Apis. So this isn't to say we won't be using tools all the time.\n    \n    But the focus will not\n    hugo bowne-anderson: not be on tool thinking. And I actually, this is maybe a provocation, and I'\n    \n    d be happy to discuss this\n    hugo bowne-anderson: with all of you. But almost an assumption we're going in with is that tools should not be the focus and tools themselves will come and go. Ways of working with tools will come and go. We're in the early days. This is the infancy of the Llm. Powered app space right?\n    \n    So\n    hugo bowne-anderson: even processes. Well, no, I'm certain some big processes will change right. But what our bet is, what won't change is a bunch of 1st principles and 1st principle, thinking for how to build software with generative AI Apps and Llms. So we'll go through those 1st principles. But that really is what this, what this course is about.\n\n5. [Distance: 1.0000] [Chunk 0] Speaker: hugo bowne-anderson. \n    [TIMESTAMP: 00:00:00.290]\n    hugo bowne-anderson: Everyone.\n    \n    hugo bowne-anderson: Hey, everyone! It's Hugo Bowne, Anderson and and Stefan Krawchik here,\n    \n    so\n    hugo bowne-anderson: so great to have you here for our 1st 1st session. So we're just gonna wait a couple of minutes. For a few more people to turn up.\n    \n    But in the meantime\n    hugo bowne-anderson: welcome. And I mean, you probably have read Stefan and my Bios, and know who who we are. But maybe I'll say a few words of introduction, and then Stefan can introduce himself.\n    \n    hugo bowne-anderson: I work as a data. And AI consultant and scientist, an educator do a lot of devrel as well. That's developer relations. So helping people who build frameworks, communicate and and educate developers. And all of all of these things. I've been doing so for over a decade now,\n    \n    and previously I was in\n    hugo bowne-anderson: scientific research, in biology, physics, math, these these types of things with my background in science and my newfound passion. Well, over the past 15 years for software. I'm just. I'm really very interested in helping our society incorporate the scientific process and software together to deliver value. All around the place, right? And really. And we'll get to a lot of this.\n    \n    What I mean by that is\n    hugo bowne-anderson: the software building skill set traditionally has been, you know, design specs build.\n    \n    hugo bowne-anderson: do some basic tests deploy and it's deterministic. So you know what works. You can have kind of very precise specs of what you want it to do, what you don't want it to do.\n    \n    hugo bowne-anderson: Then, when data, science and machine learning came into software, it was suddenly, we've got some form of entropy or stochasticity of the real world coming to these systems through data. So a lot of scientific challenges emerged there. Now with Llms and generative AI, we have a huge amount of non-determinism, you can give it the same prompt, and it will have a different result. Right? So all of these things really make the scientific approach.\n\n"
    ],
    "duration_ms": 14491.96720123291,
    "timestamp": "2025-03-28T23:05:10.433871",
    "model_provider": "gemini",
    "context_tokens": 2477,
    "completion_tokens": 179,
    "id": "q20",
    "user_type": "student",
    "scenario": "factual",
    "judgment": "pass",
    "reason": "The response directly answers the question by providing specific details about the example presented in the first workshop. It mentions the development of a foundational app for querying PDFs and generating responses, along with specific activities such as setting up the app, building the MVP, iterating on prompts, adding interactivity with Gradio, and starting logging and visualization. The response is factually correct based on the workshop content and does not include any hallucinations or fabricated information.",
    "evaluation_type": "llm"
  }
]